{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoRTyUc94S1a"
      },
      "source": [
        "# Faster R-CNN: A Classic Two-Stage Anchor-Based Object Detector\n",
        "\n",
        "In this exercise you will implement a **two-stage** object detector, based on [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf), which consists of two modules - Region Proposal Networks (RPN) and Fast R-CNN.\n",
        "Like one-stage detector in the first part of our assignment,\n",
        "we will train it to detect a set of object classes and evaluate the detection accuracy using the classic metric mean Average Precision ([mAP](https://github.com/Cartucho/mAP))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfBk3NtRgqaV"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubB_0e-UAOVK"
      },
      "source": [
        "## Setup Code\n",
        "Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You'll need to rerun this setup code each time you start the notebook.\n",
        "\n",
        "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install matplotlib==3.5\n"
      ],
      "metadata": {
        "id": "D4epUU3S8J2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASkY27ZtA7Is"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzqbYcKdz6ew"
      },
      "source": [
        "### Google Colab Setup\n",
        "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
        "\n",
        "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzRdJ3uhe1CR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab as colab\n",
        "    print('Upload the fcos.py and two_stage_detector.py file with your solution')\n",
        "    colab.files.upload()\n",
        "except:\n",
        "    print('Failed to uplaod fcos.py, try doing it manually.')\n",
        "\n",
        "# Load the library files\n",
        "!mkdir -p lib\n",
        "!wget --quiet \"https://raw.githubusercontent.com/puhsu/dl-hse/main/week03-detection2/shw3/lib/__init__.py\" -O \"lib/__init__.py\"\n",
        "!wget --quiet \"https://raw.githubusercontent.com/puhsu/dl-hse/main/week03-detection2/shw3/lib/train.py\" -O \"lib/train.py\"\n",
        "!wget --quiet \"https://raw.githubusercontent.com/puhsu/dl-hse/main/week03-detection2/shw3/lib/grad.py\" -O \"lib/grad.py\"\n",
        "!wget --quiet \"https://raw.githubusercontent.com/puhsu/dl-hse/main/week03-detection2/shw3/lib/data.py\" -O \"lib/data.py\"\n",
        "!wget --quiet \"https://raw.githubusercontent.com/puhsu/dl-hse/main/week03-detection2/shw3/lib/utils.py\" -O \"lib/utils.py\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWP1vCGL5Eca"
      },
      "source": [
        "Load several useful packages that are used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwVZ26yM5G8U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from lib.utils import *\n",
        "from lib import reset_seed\n",
        "from lib.grad import rel_error\n",
        "\n",
        "# for plotting\n",
        "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
        "plt.rcParams[\"font.size\"] = 16\n",
        "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
        "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
        "\n",
        "# To download the dataset\n",
        "!pip install wget\n",
        "\n",
        "# for mAP evaluation\n",
        "!rm -rf mAP\n",
        "!git clone https://github.com/Cartucho/mAP.git\n",
        "!rm -rf mAP/input/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7poKGI35JZY"
      },
      "source": [
        "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw3wIuCu5LnU"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Good to go!\")\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
        "    DEVICE = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjJ3uyYBg3Lw"
      },
      "source": [
        "## Load PASCAL VOC 2007 data\n",
        "As in the previous notebook, we will use PASCAL VOC 2007 dataset to train our model. The following two cells are exactly same as those in `one_stage_detector.ipynb`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIQyQe8rpDHw"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "# Set a few constants related to data loading.\n",
        "NUM_CLASSES = 20\n",
        "BATCH_SIZE = 16\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "NUM_WORKERS = multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCeZ3_5lrmvq"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget --quiet --show-progress \"https://web.eecs.umich.edu/~justincj/data/VOCtrainval_06-Nov-2007.tar\"\n",
        "!wget --quiet \"https://web.eecs.umich.edu/~justincj/data/voc07_train.json\"\n",
        "!wget --quiet \"https://web.eecs.umich.edu/~justincj/data/voc07_val.json\"\n",
        "\n",
        "!tar -xf \"VOCtrainval_06-Nov-2007.tar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmEP5KQJzk0d"
      },
      "outputs": [],
      "source": [
        "from lib.data import VOC2007DetectionTiny\n",
        "\n",
        "PATH = os.getcwd()\n",
        "train_dataset = VOC2007DetectionTiny(\n",
        "    PATH, \"train\", image_size=IMAGE_SHAPE[0],\n",
        "    download=False  # True (for the first time)\n",
        ")\n",
        "val_dataset = VOC2007DetectionTiny(PATH, \"val\", image_size=IMAGE_SHAPE[0])\n",
        "\n",
        "print(f\"Dataset sizes: train ({len(train_dataset)}), val ({len(val_dataset)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znbHrk52pDHw"
      },
      "source": [
        "Now we wrap these dataset objects with PyTorch dataloaders, similar to `one_stage_detector.ipynb`. The format of output batches will also be same as what you have seen before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNUbVtoHpDHw"
      },
      "outputs": [],
      "source": [
        "# `pin_memory` speeds up CPU-GPU batch transfer, `num_workers=NUM_WORKERS` loads data\n",
        "# on the main CPU process, suitable for Colab.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# Use batch_size = 1 during inference - during inference we do not center crop\n",
        "# the image to detect all objects, hence they may be of different size. It is\n",
        "# easier and less redundant to use batch_size=1 rather than zero-padding images.\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "train_loader_iter = iter(train_loader)\n",
        "image_paths, images, gt_boxes = next(train_loader_iter)\n",
        "\n",
        "print(f\"image paths           : {image_paths}\")\n",
        "print(f\"image batch has shape : {images.shape}\")\n",
        "print(f\"gt_boxes has shape    : {gt_boxes.shape}\")\n",
        "\n",
        "print(f\"Five boxes per image  :\")\n",
        "print(gt_boxes[:, :5, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4WmocEyiXWa"
      },
      "source": [
        "## Visualize PASCAL VOC 2007\n",
        "\n",
        "We will visualize a few images and their GT boxes, just to make sure that everything is loaded properly. You would have already seen these visualizations (and the code snippet below) in `one_stage_detector.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld1s28Z4fyL5"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from lib.utils import detection_visualizer\n",
        "\n",
        "inverse_norm = transforms.Compose(\n",
        "    [\n",
        "        transforms.Normalize(mean=[0., 0., 0.], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
        "        transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "for idx, (_, image, gt_boxes) in enumerate(train_dataset):\n",
        "    if idx > 2:\n",
        "        break\n",
        "\n",
        "    image = inverse_norm(image)\n",
        "    is_valid = gt_boxes[:, 4] >= 0\n",
        "    detection_visualizer(image, val_dataset.idx_to_class, gt_boxes[is_valid])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw0LYTy3pDHx"
      },
      "source": [
        "## Backbone with Feature Pyramid Networks (FPN)\n",
        "\n",
        "Faster R-CNN uses a convolutional backbone with FPN in the exact same way as you implemented in FCOS. So you can directly re-use it for this part of the assignment.\n",
        "\n",
        "**NOTE:** Typical state-of-the-art detectors based o nFaster R-CNN use four multi-scale features from different FPN levels — `(p2, p3, p4, p5)` with strides `(4, 8, 16, 32)`.\n",
        "Due to computational limits of Google Colab, we will instead simply use `(p3, p4, p5)` features like FCOS.\n",
        "In all your implementations for this part, you may assume that you will receive features from these three FPN levels (and may hard-code these names as Python strings). Your code will not be tested with `p2` FPN features and you will not lose points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_wZrNiSpDHx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from fcos import DetectorBackboneWithFPN\n",
        "from two_stage_detector import RPNPredictionNetwork\n",
        "\n",
        "\n",
        "backbone = DetectorBackboneWithFPN(out_channels=64)\n",
        "\n",
        "# Pass a batch of dummy images (random tensors) in NCHW format and observe the output.\n",
        "dummy_images = torch.randn(2, 3, 224, 224)\n",
        "\n",
        "# Collect dummy output.\n",
        "dummy_fpn_feats = backbone(dummy_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRc7P-RvRZGZ"
      },
      "source": [
        "## Faster R-CNN first stage: Region Proposal Network (RPN)\n",
        "\n",
        "We will now implement the first-stage of Faster R-CNN. It comprises a **Region Proposal Network (RPN)** that learns to predict general _object proposals_, which will then be used by the second stage to make final predictions.\n",
        "\n",
        "**RPN prediction:** An input image is passed through the backbone and we obtain its FPN feature maps `(p3, p4, p5)`.\n",
        "The RPN predicts multiple values at _every location on FPN features_. Faster R-CNN is _anchor-based_ — the model assumes that every location has multiple pre-defined boxes (called \"anchors\") and it predicts two measures per anchor, per FPN location:\n",
        "\n",
        "1. **Objectness:** The likelihood of having _any_ object inside the anchor. This is similar to classification head in FCOS, except that this is _class-agnostic_: it only performs binary foreground/background classification.\n",
        "2. **Box regression deltas:** 4-D \"deltas\" that _transform_ an anchor at that location to a ground-truth box.\n",
        "\n",
        "![pred_scores2](https://miro.medium.com/max/918/1*wB3ctS9WGNmw6pP_kjLjgg.png)\n",
        "\n",
        "**SIDE NOTE:** We will use the more common practice of predicting `k` logits and use a logistic regressor instead of `2k` scores (and 2-way softmax) as shown in Figure. This slightly reduces the number of trainable parameters.\n",
        "\n",
        "This RPN is conceptually quite similar to a one-stage detector like FCOS.\n",
        "The main differences with what you implemented in FCOS are: (1) RPN is anchor-based, and make predictions for multiple anchor boxes instead of location \"points\", (2) it performs class-agnostic object classification, and (3) it excludes centerness regression, which was inntroduced in FCOS itself, years after Faster R-CNN was published.\n",
        "\n",
        "Like we saw in FCOS, each anchor will be matched with a GT box for supervision — we will get to it shortly.\n",
        "For now, let's assume there are some `A` anchor boxes at every FPN location, and implement an RPN module.\n",
        "Structurally, this module is similar to FCOS prediction network.\n",
        "Now follow the instructions in `RPNPredictionNetwork` of `two_stage_detector.py` and implement layers to predict objectness and box regression deltas.\n",
        "Execute the following cell to test your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8biXAc_pDHy"
      },
      "outputs": [],
      "source": [
        "from two_stage_detector import RPNPredictionNetwork\n",
        "\n",
        "rpn_pred_net = RPNPredictionNetwork(\n",
        "    in_channels=64, stem_channels=[64], num_anchors=3\n",
        ")\n",
        "\n",
        "# Pass the dummy FPN feats to RPN prediction network and view its output shapes.\n",
        "dummy_rpn_obj, dummy_rpn_box = rpn_pred_net(dummy_fpn_feats)\n",
        "\n",
        "# Few expected outputs:\n",
        "# Shape of p4 RPN objectness: torch.Size([2,  588])\n",
        "# Shape of p5 RPN box deltas: torch.Size([2, 147, 4])\n",
        "\n",
        "print(f\"\\nFor dummy input images with shape: {dummy_images.shape}\")\n",
        "for level_name in dummy_fpn_feats.keys():\n",
        "    print(f\"Shape of {level_name} FPN features  : {dummy_fpn_feats[level_name].shape}\")\n",
        "    print(f\"Shape of {level_name} RPN objectness: {dummy_rpn_obj[level_name].shape}\")\n",
        "    print(f\"Shape of {level_name} RPN box deltas: {dummy_rpn_box[level_name].shape}\")\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etBYc7rbj35F"
      },
      "source": [
        "## Anchor-based Training of RPN\n",
        "\n",
        "Now that we implemented this RPN head, our goal is to have it predict _high objectness_ and _accurate box deltas_ for anchors that are likely to contain objects.\n",
        "Similar to first part of our assignment, we need to assign a target GT box to every RPN prediction for training supervision.\n",
        "\n",
        "**Recall FCOS location matching:** FCOS matched every FPN feature map location with a GT box (or marked them background), based on a heuristic that a location whether that location was _inside_ any GT Box.\n",
        "On the other hand, Faster R-CNN is anchor-based: instead of _locations_, it makes predictions with reference to some pre-defined _anchor boxes_, and matches each anchor with a single GT box if they have a high enough Intersection-over-Union (IoU).\n",
        "\n",
        "In the next few cells, we will perform the following steps, which are procedurally very similar to what you have already done with FCOS:\n",
        "\n",
        "1. **Anchor generation:** Generate a set of anchors for every location in FPN feature map.\n",
        "2. **Anchor to GT matching:** Match these anchors with GT boxes based on their IoU-overlap.\n",
        "3. **Format of box deltas:** Implement the tranformation functions to obtain _box deltas_ from GT boxes (model training supervision) and apply deltas to anchors (final proposal boxes for second stage).\n",
        "\n",
        "Let's approach these steps, one at a time.\n",
        "\n",
        "### Anchor Generation\n",
        "\n",
        "Recall that you already implemented a function to get the absolute image co-ordinates of FPN feature map locations, for FCOS — in `get_fpn_location_coords` of `common.py`.\n",
        "First we need to form multiple anchor boxes centered at these locations.\n",
        "RPN defines square anchor boxes of size `scale * stride` at every location, where `stride` is the FPN level stride, and `scale` is a hyperparameter.\n",
        "For example, anchor boxes for P5 level (`stride = 32`), with `scale = 2` will be boxes of `(64 x 64)` pixels.\n",
        "RPN also considers anchors of different aspect ratios, apart from square anchor boxes —\n",
        "follow the instructions in `generate_fpn_anchors` of `two_stage_detector.py` to implement creation of multiple anchor boxes per location.\n",
        "\n",
        "Execute the next cell to verify your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5w-EUJekJj-"
      },
      "outputs": [],
      "source": [
        "from fcos import get_fpn_location_coords\n",
        "from two_stage_detector import generate_fpn_anchors\n",
        "\n",
        "\n",
        "# Sanity check: Get 2x2 location co-ordinates of p5 (original shape is 7x7).\n",
        "locations = get_fpn_location_coords(\n",
        "    shape_per_fpn_level={\"p5\": (2, 64, 2, 2)}, strides_per_fpn_level={\"p5\": 32}\n",
        ")\n",
        "\n",
        "print(\"P5 locations:\\n\", locations[\"p5\"])\n",
        "\n",
        "# Generate anchors for these locations.\n",
        "anchors = generate_fpn_anchors(\n",
        "    locations_per_fpn_level=locations,\n",
        "    strides_per_fpn_level={\"p5\": 32},\n",
        "    stride_scale=2,\n",
        "    aspect_ratios=[0.5, 1.0, 2.0],\n",
        ")\n",
        "\n",
        "print(\"P5 anchors with different aspect ratios:\")\n",
        "print(\"P5 1:2 anchors:\\n\", anchors[\"p5\"][0::3, :], \"\\n\")\n",
        "# Expected (any ordering is fine):\n",
        "# [-29.2548,  -6.6274,  61.2548,  38.6274]\n",
        "# [-29.2548,  25.3726,  61.2548,  70.6274]\n",
        "# [  2.7452,  -6.6274,  93.2548,  38.6274]\n",
        "# [  2.7452,  25.3726,  93.2548,  70.6274]\n",
        "\n",
        "print(\"P5 1:1 anchors:\\n\", anchors[\"p5\"][1::3, :], \"\\n\")\n",
        "# Expected (any ordering is fine):\n",
        "# [-16., -16.,  48.,  48.]\n",
        "# [-16.,  16.,  48.,  80.]\n",
        "# [ 16., -16.,  80.,  48.]\n",
        "# [ 16.,  16.,  80.,  80.]\n",
        "\n",
        "print(\"P5 2:1 anchors:\\n\", anchors[\"p5\"][2::3, :], \"\\n\")\n",
        "# Similar to 1:2 anchors, but with flipped co-ordinates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLA4oW3XpDHy"
      },
      "source": [
        "### Matching anchor boxes with GT boxes\n",
        "\n",
        "Similar to FCOS, we will now match these generated anchors with GT boxes. Faster R-CNN matches some `N` anchor boxes with `M` GT boxes by applying a simple rule:\n",
        "\n",
        "> Anchor box $N_i$ is matched with box $M_i$ if it has an IoU overlap higher than 0.6 with that box. For multiple such GT boxes, the anchor is assigned with the GT box that has the highest IoU. Note that a single ground-truth box may assign positive labels to multiple anchors.\n",
        "\n",
        "**NOTE:** _Faster R-CNN uses 0.7 default threshold_ as mentioned in the lecture slides. We use a lower threeshold to increase the number of positive matches for sampling — this helps in speeding up training in a resource constrained setting like Google Colab.\n",
        "\n",
        "Anchor boxes with `IoU < 0.3` with ALL GT boxes is assigned background GT box `(-1, -1, -1, -1, -1)`. All other anchors with IoU between `(0.3, 0.6)` are considered \"neutral\" and ignored during training. This matching differs from FCOS, which assigns ALL anchors to either object or background — the \"neutral\" Faster R-CNN anchors cause wasted computation, and removing this redundancy would overly complicate our implementation.\n",
        "\n",
        "We have implemented this matching procedure for you — see `rcnn_match_anchors_to_gt` of `two_stage_detector.py`.\n",
        "Read its documentation to understand its input/output format, it is slightly different than `fcos_match_locations_to_gt`.\n",
        "It serves the same purpose as location matching in FCOS — to define GT targets for model predictions during training.\n",
        "\n",
        "This function internally requires IoU computation between all anchors and GT boxes — which you have to implement.\n",
        "Follow the instructions in `two_stage_detector.py` to implement IoU computation, and execute the next cell for a sanity check — you should observe an error of `1e-7` or less"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK_USCuaXSzh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from two_stage_detector import iou\n",
        "\n",
        "\n",
        "boxes1 = torch.Tensor([[10, 10, 90, 90], [20, 20, 40, 40], [60, 60, 80, 80]])\n",
        "boxes2 = torch.Tensor([[10, 10, 90, 90], [60, 60, 80, 80], [30, 30, 70, 70]])\n",
        "\n",
        "expected_iou = torch.Tensor(\n",
        "    [[1.0, 0.0625, 0.25], [0.0625, 0.0, 0.052631579], [0.0625, 1.0, 0.052631579]]\n",
        ")\n",
        "result_iou = iou(boxes1, boxes2)\n",
        "\n",
        "print(\"Relative error:\", rel_error(expected_iou, result_iou))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuVlSTqzpDHz"
      },
      "source": [
        "### Visualizing matched GT boxes\n",
        "\n",
        "Now we apply our anchor matching function and visualize one GT box with a random matched positive anchor box.\n",
        "You may try different images by indexing `train_dataset` below,\n",
        "make sure to try different FPN levels as certain images may not get any matched GT boxes due to their size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPvX4TrgaLD8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from fcos import get_fpn_location_coords\n",
        "from two_stage_detector import generate_fpn_anchors, rcnn_match_anchors_to_gt\n",
        "\n",
        "\n",
        "# Sanity check: Match anchors of p4 level with GT boxes of first image\n",
        "# in the training dataset.\n",
        "_, image, gt_boxes = train_dataset[0]\n",
        "\n",
        "FPN_LEVEL = \"p4\"\n",
        "FPN_STRIDE = 16\n",
        "locations = get_fpn_location_coords(\n",
        "    shape_per_fpn_level={FPN_LEVEL: (2, 64, 224 // FPN_STRIDE, 224 // FPN_STRIDE)},\n",
        "    strides_per_fpn_level={FPN_LEVEL: FPN_STRIDE}\n",
        ")\n",
        "# Generate anchors for these locations.\n",
        "anchors = generate_fpn_anchors(\n",
        "    locations_per_fpn_level=locations,\n",
        "    strides_per_fpn_level={FPN_LEVEL: FPN_STRIDE},\n",
        "    stride_scale=8,  # Default value used by Faster R-CNN\n",
        "    aspect_ratios=[0.5, 1.0, 2.0],\n",
        ")\n",
        "\n",
        "matched_gt_boxes = rcnn_match_anchors_to_gt(\n",
        "    anchors[FPN_LEVEL], gt_boxes, iou_thresholds=(0.3, 0.6)\n",
        ")\n",
        "\n",
        "# Flatten anchors and matched boxes:\n",
        "anchors_p4 = anchors[FPN_LEVEL].view(-1, 4)\n",
        "matched_boxes_p4 = matched_gt_boxes.view(-1, 5)\n",
        "\n",
        "# Visualize one selected anchor and its matched GT box.\n",
        "# NOTE: Run this cell multiple times to see different matched anchors. For car\n",
        "# image, p3/5 will not work because the GT box was already assigned to p4.\n",
        "fg_idxs_p4 = (matched_boxes_p4[:, 4] > 0).nonzero()\n",
        "fg_idx = random.choice(fg_idxs_p4)\n",
        "\n",
        "# Combine both boxes for visualization:\n",
        "dummy_vis_boxes = [anchors_p4[fg_idx][0], matched_boxes_p4[fg_idx][0]]\n",
        "\n",
        "print(\"Unlabeled red box is positive anchor:\")\n",
        "detection_visualizer(\n",
        "    inverse_norm(image),\n",
        "    val_dataset.idx_to_class,\n",
        "    bbox=dummy_vis_boxes,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW_Zek3_dgfF"
      },
      "source": [
        "### GT Targets for box regression\n",
        "\n",
        "Now we work on the third and final component needed to train our RPN — we define transformation functions for box regression deltas.\n",
        "Recall in the first part of the assignment, you implemented two such functions for FCOS (quoting from `one_stage_detector.ipynb`):\n",
        "\n",
        "> 1. `fcos_get_deltas_from_locations`: Accepts locations (centers) and GT boxes, and returns deltas. Required for training supervision.\n",
        "> 2. `fcos_apply_deltas_to_locations`: Accepts predicted deltas and locations, and returns predicted boxes. Required during inference.\n",
        "\n",
        "Here you will implement similar transformation functions for R-CNN, albeit with a different transformation logic than FCOS. You will find these transforms in [Lecture 13, slides 68-71](https://web.eecs.umich.edu/~justincj/slides/eecs498/WI2022/598_WI2022_lecture13.pdf), follow these and implement two functions in `two_stage_detector.py`:\n",
        "\n",
        "1. `rcnn_get_deltas_from_anchors`: Accepts anchor boxes and GT boxes, and returns deltas. Required for training supervision.\n",
        "2. `rcnn_apply_deltas_to_anchors`: Accepts predicted deltas and anchor boxes, and returns predicted boxes. Required during inference.\n",
        "\n",
        "Run the following cell to check your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX2JCaOf0768"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from two_stage_detector import rcnn_get_deltas_from_anchors, rcnn_apply_deltas_to_anchors\n",
        "\n",
        "# Three hard-coded anchor boxes and GT boxes that have a fairly high overlap.\n",
        "# Add a dummy class ID = 1 indicating foreground\n",
        "input_anchors = torch.Tensor(\n",
        "    [[20, 40, 80, 90], [10, 10, 50, 50], [120, 100, 200, 200]]\n",
        ")\n",
        "input_boxes = torch.Tensor(\n",
        "    [[10, 15, 100, 115, 1], [30, 20, 40, 30, 1], [120, 100, 200, 200, 1]]\n",
        ")\n",
        "\n",
        "# Here we do a simple sanity check - getting deltas for a particular set of boxes\n",
        "# and applying them back to anchors should give us the same boxes.\n",
        "_deltas = rcnn_get_deltas_from_anchors(input_anchors, input_boxes)\n",
        "print(_deltas)\n",
        "\n",
        "output_boxes = rcnn_apply_deltas_to_anchors(_deltas, input_anchors)\n",
        "\n",
        "print(\"Rel error in reconstructed boxes:\", rel_error(input_boxes[:, :4], output_boxes.cpu()))\n",
        "\n",
        "# Another check: deltas for GT class label = -1 should be -1e8\n",
        "background_box = torch.Tensor([[-1, -1, -1, -1, -1]])\n",
        "input_anchor = torch.Tensor([[100, 100, 200, 200]])\n",
        "_deltas = rcnn_get_deltas_from_anchors(input_anchor, background_box)\n",
        "\n",
        "output_box = rcnn_apply_deltas_to_anchors(_deltas, input_anchor)\n",
        "\n",
        "print(\"Background deltas should be all -1e8  :\", _deltas)\n",
        "print(\"Output box should be -1e8 or lower    :\", output_box)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlO2IUCnt4zu"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "With all predictions assigned with GT targets, we will proceed to compute losses for training the RPN.\n",
        "Recall that you used [Focal Loss](https://arxiv.org/abs/1708.02002) for classification and L1 loss for box regression in FCOS.\n",
        "Here, you will use L1 loss for box regression, similar to FCOS.\n",
        "\n",
        "**Objectness classification loss:** Focal Loss was proposed in RetinaNet (2017) to deal with heavy class imbalance caused by \"background\". Faster R-CNN predates this paper — it dealt with class imbalance by randomly sampling roughly equal amount of foreground-background anchors to train RPN. We have implemented a very simple sampling function for you in `sample_rpn_training` function of `two_stage_detector.py` — you may directly use it while you piece all these components (coming up next).\n",
        "\n",
        "**Total loss** is the sum of both loss components _per sampled anchor_, averaged by total number of foreground + background anchors.\n",
        "\n",
        "Execute the next cell to quickly recap their usage — you have already seen these in the first part of this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSleGX9yTeo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# Sanity check: dummy predictions from model - box regression deltas and\n",
        "# objectness logits for two anchors.\n",
        "# shape: (batch_size, HWA, 4 or 1)\n",
        "dummy_pred_boxreg_deltas = torch.randn(1, 2, 4)\n",
        "dummy_pred_obj_logits = torch.randn(1, 2, 1)\n",
        "\n",
        "# Dummy deltas and objectness targets. Let the second box be background.\n",
        "# Dummy GT boxes (matched with both anchors).\n",
        "dummy_gt_deltas = torch.randn_like(dummy_pred_boxreg_deltas)\n",
        "dummy_gt_deltas[:, 1, :] = -1e8\n",
        "\n",
        "# Background objectness targets should be 0 (not -1), and foreground\n",
        "# should be 1. Neutral anchors will not occur here due to sampling.\n",
        "dummy_gt_objectness = torch.Tensor([1, 0])\n",
        "\n",
        "# Note that loss is not multiplied with 0.25 here:\n",
        "loss_box = F.l1_loss(\n",
        "    dummy_pred_boxreg_deltas, dummy_gt_deltas, reduction=\"none\"\n",
        ")\n",
        "\n",
        "# No loss for background anchors:\n",
        "loss_box[dummy_gt_deltas == -1e8] *= 0.0\n",
        "print(\"Box regression loss (L1):\", loss_box)\n",
        "\n",
        "# Now calculate objectness loss.\n",
        "loss_obj = F.binary_cross_entropy_with_logits(\n",
        "    dummy_pred_obj_logits.view(-1), dummy_gt_objectness, reduction=\"none\"\n",
        ")\n",
        "print(\"Objectness loss (BCE):\", loss_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiPfXUHPupDE"
      },
      "source": [
        "## Putting it all together: RPN module\n",
        "\n",
        "Now you will put together all the things you have implemented into the `RPN` class in `two_stage_detector.py`.\n",
        "Implement `forward` and `predict_proposals` functions of this module — you have already done most of the heavy lifting, you simply need to call all the functions in a correct way!\n",
        "Use the previous two cells as a reference to implement loss calculation in `forward()`.\n",
        "\n",
        "**TIP:** It may help if you draw analogies between the implementation logic in this module vs FCOS (`RPN.predict_proposals()` -> `FCOS.inference()`).\n",
        "\n",
        "## Overfit small data\n",
        "\n",
        "In Faster R-CNN, the RPN is trained jointly with the second-stage network.\n",
        "However, to test our RPN implementation, we will first train just the RPN — this is basically a class-agnostic FCOS without centerness.\n",
        "We will use the `train_detector` function that we used for training FCOS.\n",
        "You can read its implementation in `a4_helper.py`.\n",
        "\n",
        "The loss should generally do down, however the forward pass here is a bit slower than FCOS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTObddiog9wJ"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "from lib.train import train_detector\n",
        "from fcos import DetectorBackboneWithFPN\n",
        "from two_stage_detector import RPN\n",
        "DEVICE='cuda'\n",
        "reset_seed(0)\n",
        "\n",
        "# Take equally spaced examples from training dataset to make a subset.\n",
        "small_dataset = torch.utils.data.Subset(\n",
        "    train_dataset,\n",
        "    torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n",
        ")\n",
        "small_train_loader = torch.utils.data.DataLoader(\n",
        "    small_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# Create a wrapper module to contain backbone + RPN:\n",
        "class FirstStage(nn.Module):\n",
        "    def __init__(self, fpn_channels: int):\n",
        "        super().__init__()\n",
        "        self.backbone = DetectorBackboneWithFPN(out_channels=fpn_channels)\n",
        "        self.rpn = RPN(\n",
        "            fpn_channels=fpn_channels,\n",
        "            # Simple stem of two layers:\n",
        "            stem_channels=[fpn_channels, fpn_channels],\n",
        "            batch_size_per_image=16,\n",
        "            anchor_stride_scale=8,\n",
        "            anchor_aspect_ratios=[0.5, 1.0, 2.0],\n",
        "            anchor_iou_thresholds=(0.3, 0.6),\n",
        "        )\n",
        "\n",
        "    def forward(self, images, gt_boxes=None):\n",
        "        feats_per_fpn_level = self.backbone(images)\n",
        "        return self.rpn(feats_per_fpn_level, self.backbone.fpn_strides, gt_boxes)\n",
        "\n",
        "\n",
        "first_stage = FirstStage(fpn_channels=64).to(DEVICE)\n",
        "\n",
        "train_detector(\n",
        "    first_stage,\n",
        "    small_train_loader,\n",
        "    learning_rate=8e-3,\n",
        "    max_iters=1000,\n",
        "    log_period=20,\n",
        "    device=DEVICE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKjv6JLMRj7s"
      },
      "source": [
        "# Faster R-CNN\n",
        "\n",
        "We have implemented the first half of Faster R-CNN, i.e., RPN, which is class-agnostic. Here, we briefly describe the second half Fast R-CNN.\n",
        "\n",
        "Given a set of proposal boxes from RPN (per FPN level, per image),\n",
        "we warp each region from the correspondng map to a fixed size 7x7 by using [RoI Align](https://arxiv.org/pdf/1703.06870.pdf).\n",
        "We will use the `roi_align` function from `torchvision`. For usage instructions, see https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_align\n",
        "\n",
        "For simplicity and computational constraints of Google Colab,\n",
        "our two-stage detector here differs from a standard Faster R-CNN system in the second stage:\n",
        "In a full implementation, the second stage of the network would predict a box deltas to further refine RPN proposals.\n",
        "We omit this for simplicity and keep RPN proposal boxes as final predictions.\n",
        "Your model will definitely perform better if you add an extra box regression head in second stage.\n",
        "\n",
        "### Your implementation exercise\n",
        "\n",
        "Read `FasterRCNN` class documentation and code to understand how everything is pieced together.\n",
        "By now you have already implemented the core components of a typical object detection system - you have dealt with anchor boxes or locations (FCOS), matched them with GT boxes, supervised model with your matching, and wrote inference utilities like NMS.\n",
        "Great work!\n",
        "\n",
        "### Classification Loss: cross entropy\n",
        "\n",
        "The classification loss for second-stage is a cross entropy loss — you would have seen this in A3, and it is a multi-class extension of binary cross entropy loss used in RPN objectness classification. You may use `torch.nn.functional.cross_entropy` directly — follow instructions in Python script.\n",
        "\n",
        "Beyond these, the second stage of Faster R-CNN doesn't add anything that is conceptually new — hence your implementation exercise is fairly lightweight.\n",
        "We have implemented most of this module for you. We left out a few 3-4 line TODO blocks, only because if we wrote them, they would given away the solution for prior exercises (RPN and FCOS).\n",
        "Moreover, empty code blocks will encourage you to carefully read the remaining portions for making everything work.\n",
        "Feel free to refer/re-use your own implementation from the first part of the assignment for filling these blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFZ49wox4MYn"
      },
      "source": [
        "## Overfit small data\n",
        "\n",
        "After adding your implementation, overfit the model on a small dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbxeAJq0zc3F"
      },
      "outputs": [],
      "source": [
        "from two_stage_detector import FasterRCNN, RPN\n",
        "\n",
        "\n",
        "# Re-initialize dataset objects for independent debugging.\n",
        "small_dataset = torch.utils.data.Subset(\n",
        "    train_dataset,\n",
        "    torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n",
        ")\n",
        "small_train_loader = torch.utils.data.DataLoader(\n",
        "    small_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "FPN_CHANNELS = 64\n",
        "backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
        "rpn = RPN(\n",
        "    fpn_channels=FPN_CHANNELS,\n",
        "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
        "    batch_size_per_image=16,\n",
        "    anchor_stride_scale=8,\n",
        "    anchor_aspect_ratios=[0.5, 1.0, 2.0],\n",
        "    anchor_iou_thresholds=(0.3, 0.6),\n",
        "    pre_nms_topk=400,\n",
        "    post_nms_topk=80,\n",
        ")\n",
        "# fmt: off\n",
        "faster_rcnn = FasterRCNN(\n",
        "    backbone, rpn, num_classes=20, roi_size=(7, 7),\n",
        "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
        "    batch_size_per_image=32,\n",
        ")\n",
        "# fmt: on\n",
        "\n",
        "train_detector(\n",
        "    faster_rcnn,\n",
        "    small_train_loader,\n",
        "    learning_rate=0.001,\n",
        "    max_iters=1000,\n",
        "    log_period=10,\n",
        "    device=DEVICE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SWA1DbG47ln"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now, follow the instructions in `FasterRCNN.inference` to implement inference, similar to `FCOS.inference`.\n",
        "\n",
        "Visualize the output from the trained model on a few images by executing the next cell, the bounding boxes should be somewhat accurate. They would get even better by using a bigger model and training it for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp_Hmt-Km5bl"
      },
      "outputs": [],
      "source": [
        "from lib.train import inference_with_detector\n",
        "\n",
        "\n",
        "# Change the loader to have (batch size = 1) as required for inference.\n",
        "small_train_loader = torch.utils.data.DataLoader(\n",
        "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "inference_with_detector(\n",
        "    faster_rcnn,\n",
        "    small_train_loader,\n",
        "    val_dataset.idx_to_class,\n",
        "    score_thresh=0.2,\n",
        "    nms_thresh=0.5,\n",
        "    device=DEVICE,\n",
        "    dtype=torch.float32,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr7wNngy4oZf"
      },
      "source": [
        "## Train a net\n",
        "\n",
        "Now it's time to train the full Faster R-CNN model on a larger subset of the the training data.\n",
        "We will train for 9000 iterations; this should take about 2-3 hours on a K80 GPU.\n",
        "Note that real object detection systems typically train for 12-24 hours, distribute training over multiple GPUs, and use much faster GPUs. As such our result will be far from the state of the art, but it should give some reasonable results!\n",
        "\n",
        "(Optional) If you train the model longer (e.g., 25K+ iterations), you should see a better mAP. But make sure you revert the code back for grading purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1k1rx1f4sTE"
      },
      "outputs": [],
      "source": [
        "from two_stage_detector import RPN, FasterRCNN\n",
        "reset_seed(0)\n",
        "\n",
        "# Slightly larger detector than in above cell.\n",
        "FPN_CHANNELS = 128\n",
        "backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
        "rpn = RPN(\n",
        "    fpn_channels=FPN_CHANNELS,\n",
        "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
        "    batch_size_per_image=16,\n",
        "    pre_nms_topk=500,\n",
        "    post_nms_topk=200  # Other args from previous cell are default args in RPN.\n",
        ")\n",
        "# fmt: off\n",
        "faster_rcnn = FasterRCNN(\n",
        "    backbone, rpn, num_classes=NUM_CLASSES, roi_size=(7, 7),\n",
        "    stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
        "    batch_size_per_image=32,\n",
        ")\n",
        "# fmt: on\n",
        "\n",
        "train_detector(\n",
        "    faster_rcnn,\n",
        "    train_loader,\n",
        "    learning_rate=0.005,\n",
        "    max_iters=9000,\n",
        "    log_period=50,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "# After you've trained your model, save the weights for submission.\n",
        "weights_path = os.path.join(PATH, \"rcnn_detector.pt\")\n",
        "torch.save(faster_rcnn.state_dict(), weights_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhWZT-ztEaqm"
      },
      "source": [
        "### Inference\n",
        "\n",
        "VIsualize a few outputs from the full trained model. They may be less accurate than FCOS.\n",
        "This is expected since our Faster R-CNN model is weaker than expected: we used a smaller model, trained for short duration, and did not include box regression in the second stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7ArGiLTnHta"
      },
      "outputs": [],
      "source": [
        "# Add some imports to run this cell independently of above few cells\n",
        "# (you will need to run first few cells at the top)\n",
        "from lib.train import inference_with_detector\n",
        "from fcos import DetectorBackboneWithFPN\n",
        "from two_stage_detector import RPN, FasterRCNN\n",
        "\n",
        "\n",
        "# Re-initialize so this cell is independent from prior cells.\n",
        "# Slightly larger detector than in above cell.\n",
        "# FPN_CHANNELS = 128\n",
        "# backbone = DetectorBackboneWithFPN(out_channels=FPN_CHANNELS)\n",
        "# rpn = RPN(fpn_channels=FPN_CHANNELS, stem_channels=[FPN_CHANNELS, FPN_CHANNELS], batch_size_per_image=16)\n",
        "# faster_rcnn = FasterRCNN(\n",
        "#     backbone, rpn, num_classes=NUM_CLASSES, roi_size=(7, 7),\n",
        "#     stem_channels=[FPN_CHANNELS, FPN_CHANNELS],\n",
        "#     batch_size_per_image=32,\n",
        "# )\n",
        "# faster_rcnn.to(device=DEVICE)\n",
        "\n",
        "# weights_path = os.path.join(PATH, \"rcnn_detector.pt\")\n",
        "# faster_rcnn.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
        "\n",
        "# Prepare a small val daataset for inference:\n",
        "small_dataset = torch.utils.data.Subset(\n",
        "    val_dataset,\n",
        "    torch.linspace(0, len(val_dataset) - 1, steps=20).long()\n",
        ")\n",
        "small_val_loader = torch.utils.data.DataLoader(\n",
        "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "inference_with_detector(\n",
        "    faster_rcnn,\n",
        "    small_val_loader,\n",
        "    val_dataset.idx_to_class,\n",
        "    score_thresh=0.1,\n",
        "    nms_thresh=0.5,\n",
        "    device=DEVICE,\n",
        "    dtype=torch.float32,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETU6ev7aydIY"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluate your Faster R-CNN like FCOS.\n",
        "(**NOTE:** It is okay if your model does not perform as good as your FCOS implementation, since we didn't train for FCOS.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvDb7uwqyhAK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "inference_with_detector(\n",
        "    faster_rcnn,\n",
        "    val_loader,\n",
        "    val_dataset.idx_to_class,\n",
        "    score_thresh=0.2,\n",
        "    nms_thresh=0.5,\n",
        "    device=DEVICE,\n",
        "    dtype=torch.float32,\n",
        "    output_dir=\"mAP/input\",\n",
        ")\n",
        "!cd mAP && python main.py\n",
        "\n",
        "# This script outputs an image containing per-class AP. Display it here:\n",
        "from IPython.display import Image\n",
        "Image(filename=\"./mAP/output/mAP.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "4ef42baa288ce895b984811292da1481faa2138d6a325169bc8d9d38d49f8a2b"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}