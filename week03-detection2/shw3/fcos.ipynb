{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "BRIqwJUr2HuN",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# FCOS: A Simple One-Stage and Anchor-Free Object Detector [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/puhsu/dl-hse/blob/master/week03-detection2/shw3/fcos.ipynb)\n",
    "\n",
    "\n",
    "In this exercise you will implement a **one-stage** object detector based on [FCOS: Fully-Convolutional One-Stage Object Detection](https://arxiv.org/abs/1904.01355) and train it to detect a set of object classes.\n",
    "Our detector design is highly similar to FCOS itself, except we train a smaller model with slightly different hyperparameters to manage with limited resources on Colab.\n",
    "\n",
    "We will also evaluate the detection accuracy using the classic metric mean Average Precision ([mAP](https://github.com/Cartucho/mAP))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "LfBk3NtRgqaV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "MzqbYcKdz6ew",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You\"ll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for the mAP vizualization\n",
    "!pip -q install matplotlib==3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now restart the runtime, for changes to take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "executionInfo": {
     "elapsed": 886,
     "status": "ok",
     "timestamp": 1604338879081,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "vxFGC0fKvkGC",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "atInOFC6vmCu",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cells to prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your solution file\n",
    "try:\n",
    "    import google.colab as colab\n",
    "    print('Upload the fcos.py file with your solution')\n",
    "    colab.files.upload()\n",
    "except:\n",
    "    print('Failed to uplaod fcos.py, try doing it manually.')\n",
    "\n",
    "# Load the library files\n",
    "!mkdir -p lib\n",
    "!wget --quiet \"https://raw.githubusercontent.com/puhsu/dl-hse/main/week03-detection2/shw3/lib/__init__.py\" -O \"lib/__init__.py\"\n",
    "!wget --quiet \"https://raw.githubusercontent.com/puhsu/dl-hse/main/week03-detection2/shw3/lib/train.py\" -O \"lib/train.py\"\n",
    "!wget --quiet \"https://raw.githubusercontent.com/puhsu/dl-hse/main/week03-detection2/shw3/lib/grad.py\" -O \"lib/grad.py\"\n",
    "!wget --quiet \"https://raw.githubusercontent.com/puhsu/dl-hse/main/week03-detection2/shw3/lib/data.py\" -O \"lib/data.py\"\n",
    "!wget --quiet \"https://raw.githubusercontent.com/puhsu/dl-hse/main/week03-detection2/shw3/lib/utils.py\" -O \"lib/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset \n",
    "!wget --quiet --show-progress \"https://web.eecs.umich.edu/~justincj/data/VOCtrainval_06-Nov-2007.tar\"\n",
    "!wget --quiet \"https://web.eecs.umich.edu/~justincj/data/voc07_train.json\"\n",
    "!wget --quiet \"https://web.eecs.umich.edu/~justincj/data/voc07_val.json\"\n",
    "\n",
    "!tar -xf \"VOCtrainval_06-Nov-2007.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "OlF0J90Zvq7c",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "If everything is working correctly then running the following cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "['fcos.ipynb', 'VOCdevkit', 'util.py', 'VOCtrainval_06-Nov-2007.tar', 'person.png', 'mAP', 'fcos.py', 'lib', '.ipynb_checkpoints', 'README.org']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 26936,
     "status": "ok",
     "timestamp": 1604338905197,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "omkktd9ywDxp",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "f1594c33-9422-4a1d-b209-50ab737ecc96",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running locally uncomment the following \n",
    "PATH = os.getcwd()\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 2022WI folder and put all the files under A4 folder, then \"2022WI/A4\"\n",
    "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = './'\n",
    "# GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "\n",
    "print(os.listdir(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 34523,
     "status": "ok",
     "timestamp": 1604338912818,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "q0nvfShXwrRY",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "32444d16-9c82-41e9-b233-41a6324b245b",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "fcos_path = os.path.join('', \"fcos.py\")\n",
    "fcos_edit_time = time.ctime(\n",
    "    os.path.getmtime(fcos_path)\n",
    ")\n",
    "print(\"fcos.py last edited on %s\" % fcos_edit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "_vm7Ign8x4b8",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Load several useful packages that are used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 36661,
     "status": "ok",
     "timestamp": 1604338914983,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "HzRdJ3uhe1CR",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "fa10c472-9574-41e5-9441-abf5406473f6",
    "run_control": {
     "read_only": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from lib.utils import *\n",
    "from lib import reset_seed\n",
    "from lib.grad import rel_error\n",
    "\n",
    "# for plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "# To download the dataset\n",
    "!pip install wget\n",
    "\n",
    "# for mAP evaluation\n",
    "!rm -rf mAP\n",
    "!git clone --quiet https://github.com/Cartucho/mAP.git\n",
    "!rm -rf mAP/input/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "OvUDZWGU3VLV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 36634,
     "status": "ok",
     "timestamp": 1604338914984,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "RrAX9FOLpr9k",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "2c374ea4-d4ae-4dcb-c855-7dc0d40580ba",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "MjJ3uyYBg3Lw",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Load PASCAL VOC 2007 data\n",
    "\n",
    "In order to train and evaluate object detection models, we need a dataset where each image is annotated with a *set* of *bounding boxes*, where each box gives the category label and spatial extent of some object in the image.\n",
    "\n",
    "We will use the [PASCAL VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/) dataset, which provides annotations of this form. PASCAL VOC ran a series of yearly computer vision competitions from 2005 to 2012, predating the ImageNet challenge.\n",
    "The data from the 2007 challenge used to be one of the most popular datasets for evaluating object detection.\n",
    "It is much smaller than more recent object detection datasets such as [COCO](http://cocodataset.org/#home), and thus easier to manage in an homework assignment.\n",
    "VOC comprises annotated bounding boxes for 20 object classes:\n",
    "`[\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]`.\n",
    "\n",
    "We create a [`PyTorch Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class\n",
    "named `VOC2007DetectionTiny` in `lib` will download the PASCAL VOC 2007 dataset.\n",
    "This class returns annotations for each image as a nested set of dictionary objects.\n",
    "\n",
    "Run the following two cells to set a few config parameters and then download the train/val sets for the PASCAL VOC 2007 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Set a few constants related to data loading.\n",
    "NUM_CLASSES = 20\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "NUM_WORKERS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: You should not hard-code any of these above values in your implementation.\n",
    "\n",
    "For example, do not use the value \"224\" while performing any view operations with image features or such. We may test your implementation with different image sizes, batch sizes and dataset subsets.\n",
    "\n",
    "They would of course yield different final accuracies and that's expected, but your code should not error on Autograder.\n",
    "As with prior assignments, write dtype and device agnostic code.\n",
    "Use `DEVICE` constant like we did in previous cell.\n",
    "\n",
    "\n",
    "### Download the dataset first.\n",
    "\n",
    "We provide images and pre-processed annotation files here. They need to be in `PATH` the following directory structure:\n",
    "\n",
    "```bash\n",
    "PATH\n",
    "    - VOCdevkit/   # Extracted from `VOCtrainval_06-Nov-2007.tar\n",
    "        - VOC2007/\n",
    "            - JPEGImages/\n",
    "            # Other directories like `SegmentationClass`, `SegmentationObject` etc. are not needed\n",
    "    - voc07_train.json\n",
    "    - voc07_val.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885,
     "referenced_widgets": [
      "72cee37f094d4bf98286cf609591a065",
      "af39fda1643342619373531854ec5a6a",
      "fcb7f0f5206242f9abef713c3fcf3d59",
      "c6de2c2b7f6d43f3bc5db37780aa6bc9",
      "39f2c90a30784ccd95c885861a8ae353",
      "7698f88da6ed494886f88d808869555c",
      "ad0732df20c546ad99c079fb55fd5110",
      "98ee12dbedf24de884be7c596bf4228a"
     ]
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 99807,
     "status": "ok",
     "timestamp": 1604338978191,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "MmEP5KQJzk0d",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "86f1c3a7-925d-4df4-c855-9d50ded95ec9",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from lib.data import VOC2007DetectionTiny\n",
    "\n",
    "# NOTE: Set `download=True` for the first time when you set up Google Drive folder.\n",
    "# Turn it back to `False` later for faster execution in the future.\n",
    "# If this hangs, download and place data in your drive manually as shown above.\n",
    "train_dataset = VOC2007DetectionTiny(PATH, \"train\", image_size=IMAGE_SHAPE[0])\n",
    "val_dataset = VOC2007DetectionTiny(PATH, \"val\", image_size=IMAGE_SHAPE[0])\n",
    "\n",
    "print(f\"Dataset sizes: train ({len(train_dataset)}), val ({len(val_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "J5MjBX9bkBtA",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "To train our detector, we need to convert individual images (JPEG) and annotations (XML files) into batches of tensors. We perform this by wrapping our datasets with a PyTorch [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) object.\n",
    "We have implemented it in `lib` - you are not required to understand its implementation, however you should understand the format of its output (which we will explore next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "executionInfo": {
     "elapsed": 99806,
     "status": "ok",
     "timestamp": 1604338978192,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "XL-7Em_A1kdS",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `pin_memory` speeds up CPU-GPU batch transfer, `num_workers=NUM_WORKERS` loads data\n",
    "# on the main CPU process, suitable for Colab.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=1\n",
    ")\n",
    "\n",
    "# Use batch_size = 1 during inference - during inference we do not center crop\n",
    "# the image to detect all objects, hence they may be of different size. It is\n",
    "# easier and less redundant to use batch_size=1 rather than zero-padding images.\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, pin_memory=True, num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "HTyRHqwlC1Au",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The `DataLoader` objects return batches of data.\n",
    "\n",
    "The first output from the `DataLoader` is a Tensor `image` of shape `(B, 3, IMAGE_SHAPE[0], IMAGE_SHAPE[1])`. This is a batch of `B` images, similar to what we have seen in classification datasets.\n",
    "\n",
    "The second output from the `DataLoader` is a Tensor `gt_boxes` of shape `(B, N, 5)` giving information about all objects in all images of the batch. `gt_boxes[i, j] = (x1, y1, x2, y2, C)` gives information about the `j`th object in `image[i]`. The position of the top-left corner of the box is `(x1, y1)` and the position of the bottom-right corner of the box is `(x2, x2)`. These coordinates are real-valued in `[0, 224]`. `C` is an integer giving the category label for this bounding box. This `(x1, y1, x2, y2)` format for bounding boxes is commonly referred as XYXY format.\n",
    "\n",
    "Each image can have different numbers of objects. If `image[i]` has $N_i$ objects, then $N = \\max_i(N_i)$ is the maximum number of objects per image among all objects in the batch; this value can vary from batch to batch. For the images that have fewer than $N$ annotated objects, only the first $N_i$ rows of `gt_boxes[i]` contain annotations; the remaining rows are padded with -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 110867,
     "status": "ok",
     "timestamp": 1604338989282,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "nZVYFJD32I_l",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "57eeeb01-b02e-4e50-ccad-d180bcd9d382",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "train_loader_iter = iter(train_loader)\n",
    "image_paths, images, gt_boxes = next(train_loader_iter)\n",
    "\n",
    "print(f\"image paths           : {image_paths}\")\n",
    "print(f\"image batch has shape : {images.shape}\")\n",
    "print(f\"gt_boxes has shape    : {gt_boxes.shape}\")\n",
    "\n",
    "print(f\"Five boxes per image  :\")\n",
    "print(gt_boxes[:, :5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "X4WmocEyiXWa",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Visualize PASCAL VOC 2007\n",
    "\n",
    "Before starting to build your model, it is highly recommended that you visualize your training data and observe some examples. This can help uncover any bugs in dataloading and sometimes even give you strong intuitions to include a modeling component!\n",
    "\n",
    "We also use a function to visualize our detections, implemented in `lib/utils.py`. You are not required to understand it but we encourage you to read it! Here we sample some images from the PASCAL VOC 2007 training set, and visualize the ground-truth object boxes and category labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "executionInfo": {
     "elapsed": 111357,
     "status": "ok",
     "timestamp": 1604338989775,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "ld1s28Z4fyL5",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from lib.utils import detection_visualizer\n",
    "\n",
    "# Define an \"inverse\" transform for the image that un-normalizes by ImageNet color\n",
    "# and mean. Without this, the images will NOT be visually understandable.\n",
    "inverse_norm = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0., 0., 0.], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
    "        transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for idx, (_, image, gt_boxes) in enumerate(train_dataset):\n",
    "    # Stop after visualizing three images.\n",
    "    if idx > 2:\n",
    "        break\n",
    "\n",
    "    # Un-normalize image to bring in [0, 1] RGB range.\n",
    "    image = inverse_norm(image)\n",
    "\n",
    "    # Remove padded boxes from visualization.\n",
    "    is_valid = gt_boxes[:, 4] >= 0\n",
    "    detection_visualizer(image, val_dataset.idx_to_class, gt_boxes[is_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing FCOS\n",
    "\n",
    "FCOS is a fully-convolutional one-stage object detection model — unlike two-stage detectors like Faster R-CNN, it does not comprise any custom modules like anchor boxes, RoI pooling/align, and RPN proposals (for second stage). Due to its simplicity, you will implement core components of FCOS in this first half of the assignment, and then re-use many of them to implement Faster R-CNN in the second half.\n",
    "\n",
    "An overview of the model in shown below. In case it does not load, see [Figure 2 in FCOS paper](https://arxiv.org/abs/1904.01355).\n",
    "It details three modeling components: backbone, feature pyramid network (FPN), and head (prediction layers).\n",
    "First, we will implement FCOS as shown in this figure, and then implement components to train it with the PASCAL VOC 2007 dataset we loaded above.\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-23_at_3.34.09_PM_SAg1OBo.png\" alt=\"FCOS Model Figure\" width=\"80%\">\n",
    "\n",
    "> **CAUTION:** The original FCOS model (as per figure above, and lecture slides) places the centerness predictor in parallel with classification predictor. However, we will follow the widely prevalent implementation practice to place the centerness predictor in parallel with box regression predictor.\n",
    "The main intuition is that centerness and box regression are localization-related quantities and hence would benefit to have shared features.\n",
    "\n",
    "## Implementing Backbone and Feature Pyramid Network\n",
    "\n",
    "First, we start building the backbone and FPN of our detector (blue and green parts above). It is the core component that takes in an image and outputs its features of different scales. It can be any type of convolutional network that progressively downsamples the image (e.g. via intermediate max pooling).\n",
    "\n",
    "Here, we use a small [RegNetX-400MF](https://pytorch.org/vision/stable/models.html#torchvision.models.regnet_x_400mf) as the backbone so we can train in reasonable time on Colab. We have already implemented the minimal logic to initialize this backbone from pre-trained ImageNet weights and extract intermediate features `(c3, c4, c5)` as shown in the figure above.\n",
    "These features `(c3, c4, c5)` have height and width that is ${1/8}^{th}$, ${1/16}^{th}$, and ${1/32}^{th}$ of the input image respectively.\n",
    "These values `(8, 16, 32)` are called the \"stride\" of these features.\n",
    "In other words, it means that moving one location on the FPN level is equivalent to moving `stride` pixels in the input image.\n",
    "\n",
    "You need to implement extra modules to attach the FPN to this backbone. For more details, see Figure 3 in [FPN paper](https://arxiv.org/abs/1612.03144).\n",
    "FPN will convert these `(c3, c4, c5)` multi-scale features to `(p3, p4, p5)`. These notations \"p3\", \"p4\", \"p5\" are called _FPN levels_.\n",
    "Before you write any code, let's initialize the backbone in the next cell. You should see the shape of `(c3, c4, c5)` features for an input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bbdfd4e4a86a48fb86b2063510dd53d9",
      "876bc727f1a341868e3ef42ba2763408",
      "b7417b5be2ee4915be4dc010d170ce7c",
      "69460e1899694f06a47c55f4fd2cab38",
      "f45f1f174a9e479aa6f6e1fa61527af1",
      "9afd2271777144eea2b996de9747975c",
      "8d6a6ba882824d40a258d0eca1b70335",
      "cb76667a3a5f4e9a8100e2407b2bdae8"
     ]
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 112820,
     "status": "ok",
     "timestamp": 1604338991274,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "_pV0Lau_yDwX",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "d44e09cf-d937-4449-9be6-906de55edf14",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from fcos import DetectorBackboneWithFPN\n",
    "\n",
    "backbone = DetectorBackboneWithFPN(out_channels=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions in `common.py` to implement additional FPN layers for transforming `(c3, c4, c5)` to `(p3, p4, p5)`.\n",
    "For training a small enough model on Google Colab, we leave out `(p6, p7)` as shown in the Figure.\n",
    "Output features from these FPN levels are expected to have same height and width as backbone features, but now they should have the same number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra FPN modules added:\")\n",
    "print(backbone.fpn_params)\n",
    "\n",
    "# Pass a batch of dummy images (random tensors) in NCHW format and observe the output.\n",
    "dummy_images = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "# Collect dummy output.\n",
    "dummy_fpn_feats = backbone(dummy_images)\n",
    "\n",
    "print(f\"For dummy input images with shape: {dummy_images.shape}\")\n",
    "for level_name, feat in dummy_fpn_feats.items():\n",
    "    print(f\"Shape of {level_name} features: {feat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing FCOS prediction network (head)\n",
    "\n",
    "By this point, you have implemented the backbone and FPN — you will reuse your implementation in the second half, for Faster R-CNN.\n",
    "Now we implement the \"head\" prediction layers (**orange** blocks in the Figure above). This head has shared weights across all FPN levels, and its purpose is to predict an object class, bounding box, and centerness at every location.\n",
    "\n",
    "Look closely at the right block in the Figure which shows the inner components of head. It comprises 4-convolution layers that produce `(H, W, 256)` features (different H, W for different FPN levels) and then use one convolutional layer each to make final predictions. \n",
    "\n",
    "You will now add these modules in `FCOSPredictionNetwork` in `one_stage_detector.py` — follow instructions in its documentation.\n",
    "Below we show how to initialize and use this module.\n",
    "\n",
    "In the expected output. the classification logits have `NUM_CLASSES` channels, box regression deltas have 4 output channels, and centerness has 1 output channels.\n",
    "The height and width of all outputs is flattened to one dimension, resulting in `(B, H * W, C)` format - this format is more convenient for computing loss, as you will see later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcos import FCOSPredictionNetwork\n",
    "\n",
    "# Tiny head with `in_channels` as FPN output channels in prior cell,\n",
    "# and two conv layers in stem.\n",
    "pred_net = FCOSPredictionNetwork(\n",
    "    num_classes=NUM_CLASSES, in_channels=64, stem_channels=[64, 64]\n",
    ")\n",
    "\n",
    "print(\"FCOS prediction network parameters:\")\n",
    "print(pred_net)\n",
    "\n",
    "# Pass the dummy output from FPN (obtained in previous cell) to the head.\n",
    "dummy_preds = pred_net(dummy_fpn_feats)\n",
    "\n",
    "pred_cls_logits, pred_boxreg_deltas, pred_ctr_logits = dummy_preds\n",
    "\n",
    "print(\"Classification logits:\")\n",
    "for level_name, feat in pred_cls_logits.items():\n",
    "    print(f\"Shape of {level_name} predictions: {feat.shape}\")\n",
    "\n",
    "print(\"Box regression deltas:\")\n",
    "for level_name, feat in pred_boxreg_deltas.items():\n",
    "    print(f\"Shape of {level_name} predictions: {feat.shape}\")\n",
    "\n",
    "print(\"Centerness logits:\")\n",
    "for level_name, feat in pred_ctr_logits.items():\n",
    "    print(f\"Shape of {level_name} predictions: {feat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train your FCOS?\n",
    "\n",
    "You have now finished implementing FCOS and it is ready to train!\n",
    "Did you notice that all the custom trainable layers you implemented so far — in FPN and in prediction layers — are convolutional layers?\n",
    "Due to this design, FCOS gets _fully-convolutional_ in its name!\n",
    "It performs object detection with _only_ convolution layers, and no special anchors, proposals etc.\n",
    "The next few steps focus on preparing the training pipeline for FCOS.\n",
    "\n",
    "Recall that image classification models are trained with `(image, label)` pairs,\n",
    "where `label` is an integer (more concretely a one-hot vector) that is associated with the _entire_ image.\n",
    "You cannot train an object detector in this fashion because of two reasons:\n",
    "\n",
    "1. Each image has a variable number of bounding boxes (and their class labels).\n",
    "2. Any class label is not associated with an entire image, but rather only a small region enclosed by the bounding box.\n",
    "\n",
    "Since the object detector classifies every location in feature map along with a bounding box and centerness, we need to supervise every prediction with _one and only one_ GT target. In next few cells, you will work through this assignment procedure between predictions and GT boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning a GT target to every model prediction\n",
    "\n",
    "FCOS heads make three predictions at every location: object class, bounding box, and centerness.\n",
    "We need to assign a GT target for each of them during training. All three predictions correspond to a single location on an FPN level `(p3, p4, p5)`, so instead we can view this problem as assigning GT boxes (and their class labels) to every FPN feature map location.\n",
    "\n",
    "GT boxes are available (from the dataloader) as 5D vectors `(x1, y1, x2, y2, C)` where `(x1, y1)` is the top-left co-ordinate and `(x2, y2)` is the bottom-right co-ordinate of the bounding box, and `C` is its object class label. These co-ordinates are absolute and real-valued in image dimensions. To begin with the assignment, we will represent every location on an FPN level with `(xc, yc)` absolute and real-valued co-ordinates of a point on the image, that are centers of the receptive fields of those features.\n",
    "\n",
    "For example, given features from FPN level having shape `(batch_size, channels, H / stride, W / stride)` and the location `feature[:, :, i, j]` will map to the image pixel `(stride * (i + 0.5), stride * (j + 0.5))` - 0.5 indicates the shift from top-left corner to the center of \"stride box\".\n",
    "\n",
    "Implement the `get_fpn_location_coords` in `common.py` to get `(xc, yc)` location co-ordinates of all FPN features. Follow its documentation and see its usage example in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 114781,
     "status": "ok",
     "timestamp": 1604338993286,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "gffaPg4Dsfux",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "a661c711-b32f-4187-e7d7-d2b835c7caa5",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fcos import get_fpn_location_coords\n",
    "\n",
    "# Get shapes of each FPN level feature map. We don't call these \"dummy\" because\n",
    "# they don't depend on the _values_ of features, but rather only shapes.\n",
    "fpn_feats_shapes = {\n",
    "    level_name: feat.shape for level_name, feat in dummy_fpn_feats.items()\n",
    "}\n",
    "\n",
    "# Get CPU tensors for this sanity check: (you can pass `device=` argument.\n",
    "locations_per_fpn_level = get_fpn_location_coords(fpn_feats_shapes, backbone.fpn_strides)\n",
    "\n",
    "# First five location co-ordinates for each feature maps.\n",
    "expected_locations = {\n",
    "    \"p3\": torch.tensor([[4.0, 4.0], [4.0, 12.0], [4.0, 20.0], [4.0, 28.0], [4.0, 36.0]]),\n",
    "    \"p4\": torch.tensor([[8.0, 8.0], [8.0, 24.0], [8.0, 40.0], [8.0, 56.0], [8.0, 72.0]]),\n",
    "    \"p5\": torch.tensor([[16.0, 16.0], [16.0, 48.0], [16.0, 80.0], [16.0, 112.0], [16.0, 144.0]]),\n",
    "}\n",
    "\n",
    "print(\"First five locations per FPN level (absolute image co-ordinates):\")\n",
    "for level_name, locations in locations_per_fpn_level.items():\n",
    "    print(f\"{level_name}: {locations[:5, :].tolist()}\")\n",
    "    print(\"rel error: \", rel_error(expected_locations[level_name], locations[:5, :]))\n",
    "\n",
    "# Visualize all the locations on first image from training data.\n",
    "for level_name, locations in locations_per_fpn_level.items():\n",
    "    # Un-normalize image to bring in [0, 1] RGB range.\n",
    "    image = inverse_norm(val_dataset[0][1])\n",
    "\n",
    "    print(\"*\" * 80)\n",
    "    print(f\"All locations of the image FPN level = {level_name}\")\n",
    "    print(f\"stride = {backbone.fpn_strides[level_name]}\")\n",
    "    detection_visualizer(image, val_dataset.idx_to_class, points=locations.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** We will use \"feature map location\" and \"feature center\" interchangeably from now on, they mean the same thing — center of the receptive field of a particular feature map location at any FPN level **(yellow points above)**.\n",
    "\n",
    "### Matching feature map locations with GT boxes\n",
    "\n",
    "Now we match these locations with GT boxes for supervising our network. FCOS matches some `N` locations at any given FPN level with `M` GT boxes applying two rules:\n",
    "\n",
    "> Location $N_i$ is matched with box $M_i$ if it lies inside the box. If any location lies inside two boxes, then it is matched with the smaller box. If a location does not lie inside any box, it is assigned \"background\".\n",
    "\n",
    "> _Multi-scale matching_ for different FPN levels — for a particular FPN level, FCOS only considers a subset of boxes based on their size. Intuitively, larger boxes are assigned to `p5` and smaller boxes are assigned to `p3`.\n",
    "\n",
    "As a result of this matching, each location wil receive a bounding box and a class label (that is 5D vector `(x1, y1, x2, y2, C)`) out of `M` GT boxes, or a background `(-1, -1, -1, -1, -1)`.\n",
    "\n",
    "We have implemented this matching procedure for you, because we thought it is non-trivial for the limited time and difficulty level of this assignment.\n",
    "However, you are required to understand its input/output format and how to use it, shown in the following cell.\n",
    "We recommend you to read its implementation in `one_stage_detector.py` with name `fcos_match_locations_to_gt`. While not required for this assignment, you may refer to [Section 3.2 in FCOS paper](https://arxiv.org/abs/1904.01355)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from fcos import fcos_match_locations_to_gt\n",
    "\n",
    "# Get an image and its GT boxes from train dataset.\n",
    "_, image, gt_boxes = train_dataset[0]\n",
    "\n",
    "# Dictionary with keys {\"p3\", \"p4\", \"p5\"} and values as `(N, 5)` tensors\n",
    "# giving matched GT boxes.\n",
    "matched_boxes_per_fpn_level = fcos_match_locations_to_gt(\n",
    "    locations_per_fpn_level, backbone.fpn_strides, gt_boxes\n",
    ")\n",
    "\n",
    "# Visualize one selected location (yellow point) and its matched GT box (red).\n",
    "# Get indices of matched locations (whose class ID is not -1) from P3 level.\n",
    "FPN_LEVEL = \"p4\"\n",
    "fg_idxs_p3 = (matched_boxes_per_fpn_level[FPN_LEVEL][:, 4] != -1).nonzero()\n",
    "\n",
    "# NOTE: Run this cell multiple times to see different matched points. For car\n",
    "# image, p3/5 will not work because the one and only box was already assigned\n",
    "# to p4 due to its compatible size to p4 stride.\n",
    "_idx = random.choice(fg_idxs_p3)\n",
    "\n",
    "detection_visualizer(\n",
    "    inverse_norm(image),\n",
    "    val_dataset.idx_to_class,\n",
    "    bbox=matched_boxes_per_fpn_level[FPN_LEVEL][_idx],\n",
    "    points=locations_per_fpn_level[FPN_LEVEL][_idx]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above illustration, we can see one random location on `p3` level (**yellow**), and its matched GT box (**red**).\n",
    "With these GT targets assigned, the FCOS preiction heads are task to predict this class label (**person**) at the given location, regress the distances between this location to all box edges `(left, top, right, bottom)`, and regress a real-valued centerness at this location. \n",
    "\n",
    "From now on, it would help to think about FCOS behavior for a _single_ location, assuming that it has a matched GT box (or background). We shall now discuss and implement the output format of prediction layers and loss functions during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GT Targets for box regression\n",
    "\n",
    "The box regression head is tasked with predicting FOUR values: distances from feature locations (**yellow points**) to box edges (**red box**) as we discussed above `(left, top, right, bottom)` or simply as we could call it — `LTRB`.\n",
    "Recall that all locations and GT boxes so far are represented in absolute image co-ordinates — they range from `(0, 224)` for our input image, and can be even larger for abritrarily large input images.\n",
    "\n",
    "We cannot use this absolute co-ordinate format with our network because regressing to such large real-valued numbers will cause the gradients to explode.\n",
    "Hence, FCOS normalizes `LTRB` targets by the `stride` of FPN levels. Hence in the above example, consider FPN level P3 (`stride = 8`), the location to be `(xc, yc)` (**yellow point**) and matched GT box have co-ordinates `(x1, y1, x2, y2)`. Then the `LTRB` regression targets used to supervise the network are:\n",
    "\n",
    "    l = (xc - x1) / stride              t = (yc - y1) / stride\n",
    "    r = (x2 - xc) / stride              b = (y2 - yc) / stride\n",
    "\n",
    "These are commonly referred as \"deltas\" of box regression. Since the model is supervised to predict these during training time, one must apply an inverse transformation to these during inference to convert network outputs to predicted boxes in absolute image co-ordinates.\n",
    "\n",
    "Next, you will implement this transformation logic and its inverse in two separate functions:\n",
    "\n",
    "1. `fcos_get_deltas_from_locations`: Accepts locations (centers) and GT boxes, and returns deltas. Required for training supervision.\n",
    "2. `fcos_apply_deltas_to_locations`: Accepts predicted deltas and locations, and returns predicted boxes. Required during inference.\n",
    "\n",
    "Run the following cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 115901,
     "status": "ok",
     "timestamp": 1604338994441,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "beEUhlCHtFAN",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "b209a21a-6571-46c5-e36e-5873d4802093",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from fcos import fcos_get_deltas_from_locations, fcos_apply_deltas_to_locations\n",
    "\n",
    "# Three hard-coded input boxes and three points lying inside them.\n",
    "# Add a dummy class ID = 1 indicating foreground\n",
    "input_boxes = torch.Tensor(\n",
    "    [[10, 15, 100, 115, 1], [30, 20, 40, 30, 1], [120, 100, 200, 200, 1]]\n",
    ")\n",
    "input_locations = torch.Tensor([[30, 40], [32, 29], [125, 150]])\n",
    "\n",
    "# Here we do a simple sanity check - getting deltas for a particular set of boxes\n",
    "# and applying them back to centers should give us the same boxes. Setting a random\n",
    "# stride = 8, it should not affect reconstruction if it is same on both sides.\n",
    "_deltas = fcos_get_deltas_from_locations(input_locations, input_boxes, stride=8)\n",
    "output_boxes = fcos_apply_deltas_to_locations(_deltas, input_locations, stride=8)\n",
    "\n",
    "print(\"Rel error in reconstructed boxes:\", rel_error(input_boxes[:, :4], output_boxes))\n",
    "\n",
    "\n",
    "# Another check: deltas for GT class label = -1 should be -1.\n",
    "background_box = torch.Tensor([[-1, -1, -1, -1, -1]])\n",
    "input_location = torch.Tensor([[100, 200]])\n",
    "\n",
    "_deltas = fcos_get_deltas_from_locations(input_location, background_box, stride=8)\n",
    "output_box = fcos_apply_deltas_to_locations(_deltas, input_location, stride=8)\n",
    "\n",
    "print(\"Background deltas should be all -1    :\", _deltas)\n",
    "\n",
    "# Output box should be the location itself ([100, 200, 100, 200])\n",
    "print(\"Output box with background deltas     :\", output_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GT targets for centerness regression\n",
    "\n",
    "Given the GT deltas for a location `(left, top, right, bottom)` as computed above, FCOS defines centerness as:\n",
    "\n",
    "$$centerness = \\sqrt{\\frac{\\min(left, right) \\cdot \\min(top, bottom)}{\\max(left, right) \\cdot \\max(top, bottom)}}$$\n",
    "\n",
    "This value is maximum (1) when `left = right` and `top = bottom`, implying the center of GT box.\n",
    "At the edge of a box, one of these values will be zero, which gives zero centerness at edges.\n",
    "Centerness regression head uses these values as targets — you need to implement them next in `fcos_make_centerness_targets`.\n",
    "Execute the next cell to verify your impkementation, your relative error should be less than 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from fcos import fcos_make_centerness_targets\n",
    "\n",
    "# Three hard-coded input boxes and three points lying inside them.\n",
    "# Add a dummy class ID = 1 indicating foreground\n",
    "input_boxes = torch.Tensor(\n",
    "    [\n",
    "        [10, 15, 100, 115, 1],\n",
    "        [30, 20, 40, 30, 1],\n",
    "        [-1, -1, -1, -1, -1]  # background\n",
    "    ]\n",
    ")\n",
    "input_locations = torch.Tensor([[30, 40], [32, 29], [125, 150]])\n",
    "\n",
    "expected_centerness = torch.Tensor([0.30860671401, 0.1666666716, -1.0])\n",
    "\n",
    "_deltas = fcos_get_deltas_from_locations(input_locations, input_boxes, stride=8)\n",
    "centerness = fcos_make_centerness_targets(_deltas).squeeze()\n",
    "print(\"Rel error in centerness:\", rel_error(centerness, expected_centerness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "ah05Gd6BOKG2",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "At this point, every model prediction is assigned a GT target during training.\n",
    "We will proceed to compute losses for training the model.\n",
    "\n",
    "FCOS has three prediction layers, that use the following use functions:\n",
    "\n",
    "1. **Object classification:** FCOS uses [Focal Loss](https://arxiv.org/abs/1708.02002), an extension of cross-entropy loss that deals with class-imbalance. FCOS faces a class imbalance issue because a majority of locations would be assigned \"background\". If not handled properly, the model will simply learn to predict \"background\" for every location.\n",
    "\n",
    "2. **Box regression:** We will use a simple L1 loss to minimize the difference between predicted and GT `LTRB` deltas. FCOS uses [Generalized Intersection-over-Union](https://giou.stanford.edu/) loss, which empirically gives slightly better results but is slightly slower — we use L1 loss due to Colab time limits. (You are not required to understand GIoU for this assignment)\n",
    "\n",
    "3. **Centerness regression:** Centerness predictions and GT targets are real-valued numbers in `[0, 1]`, so FCOS uses binary cross-entropy (BCE) loss to optimize it. One may use an L1 loss, but BCE empirically works slightly better.\n",
    "\n",
    "**Total loss:** We get three loss components _per location_. Out of these, (2) and (3) are set to zero for _background locations_ because their GT boxes (and hence centerness) are not defined. Total loss is the sum of all losses per location, averaged by the number of _foreground locations_ (that matched with any GT box). The number of foreground locations are highly variable per image, depending on density of objects in it. Hence for training stability, the loss is instead average by an _exponential moving average of foreground locations_ (think like the running mean/var of BN).\n",
    "\n",
    "The following two cells demonstrate the use of the loss functions with dummy inputs and targets.\n",
    "Execute them and read them carefully so you understand how to call these functions — immediately after, you will use them with actual predictions of FCOS, and assigned GT targets (that you implemented so far)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 121548,
     "status": "ok",
     "timestamp": 1604339000233,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "B0iG-DAUOQ56",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "d2aee15f-1a4e-4f1f-827f-a2f1aae9990a",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "\n",
    "# Sanity check: dummy model predictions for TWO locations, and\n",
    "# NUM_CLASSES = 5 (typically there are thousands of locations\n",
    "# across all FPN levels).\n",
    "# shape: (batch_size, num_locations, num_classes)\n",
    "dummy_pred_cls_logits = torch.randn(1, 2, 5)\n",
    "\n",
    "# Corresponding one-hot vectors of GT class labels (2, -1), one\n",
    "# foreground and one background.\n",
    "# shape: (batch_size, num_locations, num_classes)\n",
    "dummy_gt_classes = torch.Tensor([[[0, 0, 1, 0, 0], [0, 0, 0, 0, 0]]])\n",
    "\n",
    "# This loss expects logits, not probabilities (DO NOT apply sigmoid!)\n",
    "cls_loss = sigmoid_focal_loss(\n",
    "    inputs=dummy_pred_cls_logits, targets=dummy_gt_classes\n",
    ")\n",
    "print(\"Classification loss (dummy inputs/targets):\")\n",
    "print(cls_loss)\n",
    "\n",
    "print(f\"Total classification loss (un-normalized): {cls_loss.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from fcos import fcos_get_deltas_from_locations\n",
    "\n",
    "\n",
    "# Sanity check: dummy model predictions for TWO locations, and\n",
    "# NUM_CLASSES = 2 (typically there are thousands of locations\n",
    "# across all FPN levels).\n",
    "# Think of these as first two locations locations of \"p5\" level.\n",
    "dummy_locations = torch.Tensor([[32, 32], [64, 32]])\n",
    "dummy_gt_boxes = torch.Tensor(\n",
    "    [\n",
    "        [1, 2, 40, 50, 2],\n",
    "        [-1, -1, -1, -1, -1]  # Same GT classes as above cell.\n",
    "    ]\n",
    ")\n",
    "# Centerness is just a dummy value:\n",
    "dummy_gt_centerness = torch.Tensor([0.6, -1])\n",
    "\n",
    "# shape: (batch_size, num_locations, 4 or 1)\n",
    "dummy_pred_boxreg_deltas = torch.randn(1, 2, 4)\n",
    "dummy_pred_ctr_logits = torch.randn(1, 2, 1)\n",
    "\n",
    "# Collapse batch dimension.\n",
    "dummy_pred_boxreg_deltas = dummy_pred_boxreg_deltas.view(-1, 4)\n",
    "dummy_pred_ctr_logits = dummy_pred_ctr_logits.view(-1)\n",
    "\n",
    "# First calculate box reg loss, comparing predicted boxes and GT boxes.\n",
    "dummy_gt_deltas = fcos_get_deltas_from_locations(\n",
    "    dummy_locations, dummy_gt_boxes, stride=32\n",
    ")\n",
    "# Multiply with 0.25 to average across four LTRB components.\n",
    "loss_box = 0.25 * F.l1_loss(\n",
    "    dummy_pred_boxreg_deltas, dummy_gt_deltas, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# No loss for background:\n",
    "loss_box[dummy_gt_deltas < 0] *= 0.0\n",
    "print(\"Box regression loss (L1):\", loss_box)\n",
    "\n",
    "\n",
    "# Now calculate centerness loss.\n",
    "centerness_loss = F.binary_cross_entropy_with_logits(\n",
    "    dummy_pred_ctr_logits, dummy_gt_centerness, reduction=\"none\"\n",
    ")\n",
    "# No loss for background:\n",
    "centerness_loss[dummy_gt_centerness < 0] *= 0.0\n",
    "print(\"Centerness loss (BCE):\", centerness_loss)\n",
    "\n",
    "# In the expected loss, the first value will be different everytime due to random dummy\n",
    "# predictions. But the second value should always be zero - corresponding to background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "7yCYzKIxx2qB",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Object detection module\n",
    "\n",
    "We will now combine everything into the `FCOS` class in `one_stage_detector.py`.\n",
    "Implement the `__init__` and `forward` functions of this module — you have already done most of the heavy lifting, you simply need to call the functions in a correct way!\n",
    "Use the previous two cells as a reference to implement loss calculation in `forward()`.\n",
    "\n",
    "## Overfit small data\n",
    "\n",
    "To make sure that everything is working as expected, we can try to overfit the detector to a small subset of data.\n",
    "The training loss may increase at first (due to EMA loss averaging) but then should generally go down.\n",
    "We also implemented the `train_detector` function which runs the training loop for a detector.\n",
    "You can read its implementation in `a4_helper.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import train_detector\n",
    "from fcos import FCOS\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# Take equally spaced examples from training dataset to make a subset.\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    train_dataset,\n",
    "    torch.linspace(0, len(train_dataset) - 1, steps=BATCH_SIZE * 10).long()\n",
    ")\n",
    "small_train_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "detector = FCOS(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    fpn_channels=64,\n",
    "    stem_channels=[64, 64],\n",
    ")\n",
    "detector = detector.to(DEVICE)\n",
    "\n",
    "train_detector(\n",
    "    detector,\n",
    "    small_train_loader,\n",
    "    learning_rate=5e-3,\n",
    "    max_iters=500,\n",
    "    log_period=20,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "CuSBfcGWyHlD",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Train a net\n",
    "\n",
    "Now that we are confident that the training code is working properly, let's train the network on more data and for longer. We will train for 9000 iterations; this should take about 30 minutes on a K80 GPU. For initial debugging, you may want to train for lesser durations (say 1000 iterations).\n",
    "\n",
    "Note that real object detection systems typically train for 12-24 hours, distribute training over multiple GPUs, and use much faster GPUs. As such our result will be far from the state of the art, but it should give some reasonable results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1305188,
     "status": "ok",
     "timestamp": 1604344026014,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "Aipf7-XQyJ28",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "2a180cca-46b0-4951-87ca-e903646e20d5",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fcos import FCOS\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# Slightly larger detector than in above cell.\n",
    "detector = FCOS(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    fpn_channels=128,\n",
    "    stem_channels=[128, 128],\n",
    ")\n",
    "detector = detector.to(DEVICE)\n",
    "\n",
    "train_detector(\n",
    "    detector,\n",
    "    train_loader,\n",
    "    learning_rate=8e-3,\n",
    "    max_iters=9000,\n",
    "    log_period=100,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# After you've trained your model, save the weights for submission.\n",
    "weights_path = os.path.join(PATH, \"fcos_detector.pt\")\n",
    "torch.save(detector.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "e42TAEcpjeKW",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Non-Maximum Suppression (NMS)\n",
    "\n",
    "The definition of NMS and instructions on how to compute NMS can be found in the lecture 2 slides (10-11):\n",
    "https://github.com/puhsu/dl-hse/blob/main/week02-detection/DL_2CV_04objectdetection_lec01.pdf\n",
    "\n",
    "Implement the `nms` function in `common.py`. We then compare your implementation of NMS with the implementation in torchvision. Most likely, your implementation will be faster on CPU than on CUDA, and the torchvision implementation will likely be much faster than yours.\n",
    "This is expected, but your implementation should produce the same outputs as the torchvision version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1474,
     "status": "ok",
     "timestamp": 1604344103435,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "wqXkUdvdHh-U",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "5ece168a-25f2-4b8f-d31a-5ad281b8c772",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform imports here to make this cell runnble independently,\n",
    "# students are likely to spend good mount of time here and it is\n",
    "# best to not require execution of prior cells.\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from fcos import nms\n",
    "from lib import reset_seed\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "\n",
    "boxes = (100.0 * torch.rand(5000, 4)).round()\n",
    "boxes[:, 2] = boxes[:, 2] + boxes[:, 0] + 1.0\n",
    "boxes[:, 3] = boxes[:, 3] + boxes[:, 1] + 1.0\n",
    "scores = torch.randn(5000)\n",
    "\n",
    "names = [\"your_cpu\", \"torchvision_cpu\", \"torchvision_cuda\"]\n",
    "iou_thresholds = [0.3, 0.5, 0.7]\n",
    "elapsed = dict(zip(names, [0.0] * len(names)))\n",
    "intersects = dict(zip(names[1:], [0.0] * (len(names) - 1)))\n",
    "\n",
    "for iou_threshold in iou_thresholds:\n",
    "    tic = time.time()\n",
    "    my_keep = nms(boxes, scores, iou_threshold)\n",
    "    elapsed[\"your_cpu\"] += time.time() - tic\n",
    "\n",
    "    tic = time.time()\n",
    "    tv_keep = torchvision.ops.nms(boxes, scores, iou_threshold)\n",
    "    elapsed[\"torchvision_cpu\"] += time.time() - tic\n",
    "    intersect = len(set(tv_keep.tolist()).intersection(my_keep.tolist())) / len(tv_keep)\n",
    "    intersects[\"torchvision_cpu\"] += intersect\n",
    "\n",
    "    tic = time.time()\n",
    "    tv_cuda_keep = torchvision.ops.nms(boxes.to(device=DEVICE), scores.to(device=DEVICE), iou_threshold).to(\n",
    "        my_keep.device\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed[\"torchvision_cuda\"] += time.time() - tic\n",
    "    intersect = len(set(tv_cuda_keep.tolist()).intersection(my_keep.tolist())) / len(\n",
    "        tv_cuda_keep\n",
    "    )\n",
    "    intersects[\"torchvision_cuda\"] += intersect\n",
    "\n",
    "for key in intersects:\n",
    "    intersects[key] /= len(iou_thresholds)\n",
    "\n",
    "# You should see < 1% difference\n",
    "print(\"Testing NMS:\")\n",
    "print(\"Your        CPU  implementation: %fs\" % elapsed[\"your_cpu\"])\n",
    "print(\"torchvision CPU  implementation: %fs\" % elapsed[\"torchvision_cpu\"])\n",
    "print(\"torchvision CUDA implementation: %fs\" % elapsed[\"torchvision_cuda\"])\n",
    "print(\"Speedup CPU : %fx\" % (elapsed[\"your_cpu\"] / elapsed[\"torchvision_cpu\"]))\n",
    "print(\"Speedup CUDA: %fx\" % (elapsed[\"your_cpu\"] / elapsed[\"torchvision_cuda\"]))\n",
    "print(\n",
    "    \"Difference CPU : \", 1.0 - intersects[\"torchvision_cpu\"]\n",
    ")  # in the order of 1e-3 or less\n",
    "print(\n",
    "    \"Difference CUDA: \", 1.0 - intersects[\"torchvision_cuda\"]\n",
    ")  # in the order of 1e-3 or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "9JSTPMsqzEnr",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now, follow the instructions in `FCOS.inference` to implement the core components for inference.\n",
    "\n",
    "Visualize the output from the trained model on a few eval images by running the code below, the bounding boxes should be somewhat accurate. They would get even better by using a bigger model and training it for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1b1NLbE8JiqblTfMt75oJA_2rmoCf_scm"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 4559,
     "status": "ok",
     "timestamp": 1604344112451,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "gp_Hmt-Km5bl",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "d2ea2123-0384-498b-c0c0-0829ae4dba65",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from lib import inference_with_detector\n",
    "from fcos import FCOS\n",
    "\n",
    "\n",
    "weights_path = os.path.join(PATH, \"fcos_detector.pt\")\n",
    "\n",
    "# Re-initialize so this cell is independent from prior cells.\n",
    "detector = FCOS(\n",
    "    num_classes=NUM_CLASSES, fpn_channels=64, stem_channels=[64,64]\n",
    ")\n",
    "detector.to(device=DEVICE)\n",
    "detector.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
    "\n",
    "# Prepare a small val daataset for inference:\n",
    "small_dataset = torch.utils.data.Subset(\n",
    "    val_dataset,\n",
    "    torch.linspace(0, len(val_dataset) - 1, steps=10).long()\n",
    ")\n",
    "small_val_loader = torch.utils.data.DataLoader(\n",
    "    small_dataset, batch_size=1, pin_memory=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "inference_with_detector(\n",
    "    detector,\n",
    "    small_val_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.5,\n",
    "    nms_thresh=0.5,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "ETU6ev7aydIY",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Evaluation\n",
    "Compute mean Average Precision (mAP). Introduction on mAP see lecture 2 slide 7: https://github.com/puhsu/dl-hse/blob/main/week02-detection/DL_2CV_04objectdetection_lec01.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "1fGptrealquF",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Run the following to evaluate your detector on the PASCAL VOC validation set. Evaluation should take a few minutes, and with default hyperparameters declared above, you should see at least 35% mAP.\n",
    "\n",
    "The state of the art on this dataset is >80% mAP! To achieve these results we would need to use a much bigger network, and train with more data and for much longer, but that is beyond the scope of this assigment. **Optional:** For better mAP, you may use more conv layers in head stem, train for 25K+ iterations, and use ResNet-50/RegNet-4GF models in backbone.\n",
    "But make sure you revert the code back for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 41965,
     "status": "ok",
     "timestamp": 1604344169809,
     "user": {
      "displayName": "Mohamed El Banani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GggOGiYfSuGtZ3nDZTHgI1FZ7khEDL9VZbcpo8=s64",
      "userId": "10640812476884023238"
     },
     "user_tz": 300
    },
    "id": "FvDb7uwqyhAK",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "e8024dfc-1131-4a3d-a629-3f8997bfb7de",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "inference_with_detector(\n",
    "    detector,\n",
    "    val_loader,\n",
    "    val_dataset.idx_to_class,\n",
    "    score_thresh=0.4,\n",
    "    nms_thresh=0.6,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,\n",
    "    output_dir=\"mAP/input\",\n",
    ")\n",
    "!cd mAP && python main.py\n",
    "\n",
    "# This script outputs an image containing per-class AP. Display it here:\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./mAP/output/mAP.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fcos.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "4ef42baa288ce895b984811292da1481faa2138d6a325169bc8d9d38d49f8a2b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "39f2c90a30784ccd95c885861a8ae353": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "69460e1899694f06a47c55f4fd2cab38": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb76667a3a5f4e9a8100e2407b2bdae8",
      "placeholder": "​",
      "style": "IPY_MODEL_8d6a6ba882824d40a258d0eca1b70335",
      "value": " 13.6M/13.6M [00:00&lt;00:00, 17.5MB/s]"
     }
    },
    "72cee37f094d4bf98286cf609591a065": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fcb7f0f5206242f9abef713c3fcf3d59",
       "IPY_MODEL_c6de2c2b7f6d43f3bc5db37780aa6bc9"
      ],
      "layout": "IPY_MODEL_af39fda1643342619373531854ec5a6a"
     }
    },
    "7698f88da6ed494886f88d808869555c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "876bc727f1a341868e3ef42ba2763408": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d6a6ba882824d40a258d0eca1b70335": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98ee12dbedf24de884be7c596bf4228a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9afd2271777144eea2b996de9747975c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad0732df20c546ad99c079fb55fd5110": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af39fda1643342619373531854ec5a6a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7417b5be2ee4915be4dc010d170ce7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9afd2271777144eea2b996de9747975c",
      "max": 14212972,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f45f1f174a9e479aa6f6e1fa61527af1",
      "value": 14212972
     }
    },
    "bbdfd4e4a86a48fb86b2063510dd53d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b7417b5be2ee4915be4dc010d170ce7c",
       "IPY_MODEL_69460e1899694f06a47c55f4fd2cab38"
      ],
      "layout": "IPY_MODEL_876bc727f1a341868e3ef42ba2763408"
     }
    },
    "c6de2c2b7f6d43f3bc5db37780aa6bc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98ee12dbedf24de884be7c596bf4228a",
      "placeholder": "​",
      "style": "IPY_MODEL_ad0732df20c546ad99c079fb55fd5110",
      "value": " 460038144/? [01:10&lt;00:00, 9269400.25it/s]"
     }
    },
    "cb76667a3a5f4e9a8100e2407b2bdae8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f45f1f174a9e479aa6f6e1fa61527af1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fcb7f0f5206242f9abef713c3fcf3d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7698f88da6ed494886f88d808869555c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39f2c90a30784ccd95c885861a8ae353",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
