{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPNjG3uCLC8m"
   },
   "source": [
    "# **Отладка нейронных сетей**\n",
    "\n",
    "TL;DR: нужно найти и исправить всё что не так.\n",
    "\n",
    "Правила игры:\n",
    "- Нельзя добавлять параметров в сеть---нельзя менять параметры количество/параметры свёрточных и линейных слоёв.\n",
    "- Читать примеры обучения сетки на пайторче/отдельные куски на стековерфлоу против правил.\n",
    "- Можно читать документацию к функциям :)\n",
    "- Количество эпох увеличивать нельзя. *25 эпох хватит всем*.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/puhsu/dl-hse/blob/master/week01-intro/looking_for_bugs.ipynb)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/teaching/dasha.jpg\" style=\"width: 50%;\">\n",
    "\n",
    "# Будем работать с датасетом Fashion-MNIST\n",
    "\n",
    "Подробнее о датасете [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/teaching/fmnist.png\" style=\"width: 80%;\">\n",
    "\n",
    "<!-- <img src=\"https://miro.medium.com/max/2312/1*jXssb_WjoYZgepOCfdQfJA.png\" style=\"width:10%\"> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1Nyr9U0TBIP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tabulate import tabulate\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIbgJhyisH5R"
   },
   "outputs": [],
   "source": [
    "# bug free, I swear\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, name='name', fmt=None):\n",
    "        self.handler = True\n",
    "        self.scalar_metrics = OrderedDict()\n",
    "        self.fmt = fmt if fmt else dict()\n",
    "\n",
    "        base = './logs'\n",
    "        if not os.path.exists(base): os.mkdir(base)\n",
    "\n",
    "        time = gmtime()\n",
    "        hash = ''.join([chr(random.randint(97, 122)) for _ in range(5)])\n",
    "        fname = '-'.join(sys.argv[0].split('/')[-3:])\n",
    "        self.path = '%s/%s-%s-%s-%s' % (base, fname, name, strftime('%m-%d-%H-%M', time), hash)\n",
    "\n",
    "        self.logs = self.path + '.csv'\n",
    "        self.output = self.path + '.out'\n",
    "        self.checkpoint = self.path + '.cpt'\n",
    "\n",
    "    def print(self, *args):\n",
    "        str_to_write = ' '.join(map(str, args))\n",
    "        with open(self.output, 'a') as f:\n",
    "            f.write(str_to_write + '\\n')\n",
    "            f.flush()\n",
    "\n",
    "        print(str_to_write)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def add_scalar(self, t, key, value):\n",
    "        if key not in self.scalar_metrics:\n",
    "            self.scalar_metrics[key] = []\n",
    "        self.scalar_metrics[key] += [(t, value)]\n",
    "\n",
    "    def add_dict(self, t, d):\n",
    "        for key, value in d.iteritems():\n",
    "            self.add_scalar(t, key, value)\n",
    "\n",
    "    def add(self, t, **args):\n",
    "        for key, value in args.items():\n",
    "            self.add_scalar(t, key, value)\n",
    "\n",
    "    def iter_info(self, order=None):\n",
    "        names = list(self.scalar_metrics.keys())\n",
    "        if order:\n",
    "            names = order\n",
    "        values = [self.scalar_metrics[name][-1][1] for name in names]\n",
    "        t = int(np.max([self.scalar_metrics[name][-1][0] for name in names]))\n",
    "        fmt = ['%s'] + [self.fmt[name] if name in self.fmt else '.1f' for name in names]\n",
    "\n",
    "        if self.handler:\n",
    "            self.handler = False\n",
    "            self.print(tabulate([[t] + values], ['epoch'] + names, floatfmt=fmt))\n",
    "        else:\n",
    "            self.print(tabulate([[t] + values], ['epoch'] + names, tablefmt='plain', floatfmt=fmt).split('\\n')[1])\n",
    "\n",
    "    def save(self, silent=False):\n",
    "        result = None\n",
    "        for key in self.scalar_metrics.keys():\n",
    "            if result is None:\n",
    "                result = DataFrame(self.scalar_metrics[key], columns=['t', key]).set_index('t')\n",
    "            else:\n",
    "                df = DataFrame(self.scalar_metrics[key], columns=['t', key]).set_index('t')\n",
    "                result = result.join(df, how='outer')\n",
    "        result.to_csv(self.logs)\n",
    "        if not silent:\n",
    "            self.print('The log/output/model have been saved to: ' + self.path + ' + .csv/.out/.cpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvVOw588TClN"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout1 = nn.Dropout2d(0.9)\n",
    "        self.dropout2 = nn.Dropout2d(0.9)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNzViLZkF2J9"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.constant_(m.weight.data, 0)\n",
    "        torch.nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcGjNVjNT4Oe"
   },
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer):\n",
    "    global logger, epoch\n",
    "    mean_loss = 0.0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mean_loss += len(data)/len(train_loader.dataset) * loss.item()\n",
    "        with torch.no_grad():\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    logger.add_scalar(epoch, 'train_loss',  mean_loss)\n",
    "    logger.add_scalar(epoch, 'train_acc',  100. * correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpAnu_aZT9JA"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    global logger, epoch\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    logger.add_scalar(epoch, 'test_loss', test_loss)\n",
    "    logger.add_scalar(epoch, 'test_acc',  100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PL2k9h-UAed"
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = {\n",
    "  'batch_size': 128,\n",
    "  'test_batch_size': 1000,\n",
    "  'epochs': 25,\n",
    "  'lr': 1e-3,\n",
    "  'gamma': 0.1,\n",
    "  'no_cuda': False,\n",
    "  'log_interval': 2,\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "print('args:', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBLCUqsityS7"
   },
   "outputs": [],
   "source": [
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9BYAqi86n2_"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.RandomResizedCrop(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    '../data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=5,\n",
    "    batch_size=args.batch_size)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    '../data', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=5,\n",
    "    batch_size=args.test_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jexD4-SQUpnI"
   },
   "outputs": [],
   "source": [
    "fmt = {'test_loss': '.3f', 'test_acc': '.3f', 'train_loss': '.3f', 'lr': '1.1e'}\n",
    "logger = Logger('sparse_vd', fmt=fmt)\n",
    "\n",
    "model = Net().to(device).train()\n",
    "model = model.apply(weights_init)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    logger.add_scalar(epoch, 'lr', scheduler.get_last_lr()[0])\n",
    "    train(args, model, device, train_loader, optimizer)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "    if epoch % args.log_interval == 0 or epoch==1:\n",
    "      logger.iter_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45Kb53haewpf"
   },
   "outputs": [],
   "source": [
    "# Plot train and test loss\n",
    "train_loss = np.array(logger.scalar_metrics['train_loss'])[:,1]\n",
    "plt.plot(train_loss, 'o-', label='train_loss')\n",
    "\n",
    "test_loss = np.array(logger.scalar_metrics['test_loss'])[:,1]\n",
    "plt.plot(test_loss, 'o-', label='test_loss')\n",
    "\n",
    "plt.plot([0.21]*len(train_loss), '-', label='you win (for test)', c='r')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('# epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvSSJd2bnUUR"
   },
   "outputs": [],
   "source": [
    "# Plot train and test accuracy\n",
    "train_acc = np.array(logger.scalar_metrics['train_acc'])[:,1]\n",
    "plt.plot(train_acc, 'o-', label='train_acc')\n",
    "\n",
    "test_acc = np.array(logger.scalar_metrics['test_acc'])[:,1]\n",
    "plt.plot(test_acc, 'o-', label='test_acc')\n",
    "\n",
    "plt.plot([92]*len(train_acc), '-', label='you win (for test)', c='r')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('# epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwXL0Sayq4FI"
   },
   "outputs": [],
   "source": [
    "# Plot learning rate\n",
    "train_loss = np.array(logger.scalar_metrics['lr'])[:,1]\n",
    "plt.plot(train_loss, 'o-', label='lr')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('# epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C8K1_JFrSpH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
