{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436319bd",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "**Author**: Oleg Platonov\n",
    "\n",
    "Graph Neural Networks (GNNs) are currently the most popular approach to machine learning on graphs. Many GNN architectures can be unified by the Message-Passing Neural Networks (MPNNs) framework. Below we will describe (a variant of) this framework and implement and train several examples of MPNNs.\n",
    "\n",
    "First, let's introduce the notation we will be using in this notebook. Let $G = (V, E)$ be a graph with nodeset $V$ and edgeset $E$, $|V| = n$, $|E| = m$. Let $N(v)$ be the one-hop neighborhood of the node $v$ and $deg(v)$ be the degree of node $v$, $deg(v) = |N(v)|$. Let $A$ be the adjacency matrix of graph $G$ and $D$ be the diagonal degree matrix of graph $G$, i.e., $D = diag \\Big( deg(v_1), \\; deg(v_2), \\; ..., \\; deg(v_n) \\Big)$.\n",
    "\n",
    "In each layer $l$ an MPNN creates a representation $h_i^l$ of each node $v_i$ from it's previous-layer representation and previous-layer representations of its neighbors using the following formula:\n",
    "\n",
    "$$ h_i^{l+1} = \\mathrm{Update} \\Bigg( h_i^l, \\; \\mathrm{Aggregate} \\Big( \\Big\\{ (h_i^l, \\; h_j^l): \\; v_j \\in N(v_i) \\Big\\} \\Big) \\Bigg) $$\n",
    "\n",
    "Here, $\\mathrm{Aggregate}$ is a function that aggregates information from the set of neighbors (since it operates on a set, it should be invariant to the order of neighbors) and $\\mathrm{Update}$ is a function that combines the node's previous-layer representation with the aggregated information from its neighbors. For example, $\\mathrm{Aggregate}$ can be the elementwise mean operation over the set of neighbors and $\\mathrm{Update}$ can be an MLP that takes two concatenated vectors as input:\n",
    "\n",
    "$$ h_i^{l+1} = \\mathrm{MLP} \\Bigg( \\bigg[ h_i^l \\; \\mathbin\\Vert \\; \\mathrm{mean} \\Big( \\Big\\{ h_j^l: \\; v_j \\in N(v_i) \\Big\\} \\Big) \\bigg] \\Bigg) $$\n",
    "\n",
    "(this is actually the first GNN that we will implement in this seminar).\n",
    "\n",
    "The $\\mathrm{Aggregate}$ operation is often called graph convolution\n",
    "\n",
    "Note that variations of the above MPNN formula are possible. For example, edge representations can be added, but we won't do it in this seminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc361f",
   "metadata": {},
   "source": [
    "Now, let's get us a graph. PyTorch Geometric library provides a lot of popular graph datasets. We will use the Amazon-Computers dataset. It is a co-purchasing network where nodes represent products, edges indicate that two products are frequently bought together, node features are bag-of-words-encoded product reviews, and node labels are product categories. The graph is a simple undirected graph without self-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73406c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab958983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.Amazon(name='computers', root='data')[0]\n",
    "features = data.x\n",
    "labels = data.y\n",
    "edges = data.edge_index.T\n",
    "\n",
    "print(f'Number of nodes: {len(labels)}')\n",
    "print(f'Number of edges: {len(edges)}')\n",
    "print(f'Average node degree: {len(edges) * 2 / len(labels):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05695fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40cf2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_idx = np.arange(len(labels))\n",
    "train_idx, val_and_test_idx = train_test_split(full_idx, test_size=0.5, random_state=0,\n",
    "                                               stratify=labels)\n",
    "\n",
    "val_idx, test_idx = train_test_split(val_and_test_idx, test_size=0.5, random_state=0,\n",
    "                                     stratify=labels[val_and_test_idx])\n",
    "\n",
    "train_idx = torch.from_numpy(train_idx)\n",
    "val_idx = torch.from_numpy(val_idx)\n",
    "test_idx = torch.from_numpy(test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d27aa1",
   "metadata": {},
   "source": [
    "Let's prepare a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, scaler, amp, graph, features, labels, train_idx):\n",
    "    model.train()\n",
    "\n",
    "    with autocast(enabled=amp):\n",
    "        logits = model(graph=graph, x=features)\n",
    "        loss = F.cross_entropy(input=logits[train_idx], target=labels[train_idx])\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, amp, graph, features, labels, train_idx, test_idx, val_idx):\n",
    "    model.eval()\n",
    "\n",
    "    with autocast(enabled=amp):\n",
    "        logits = model(graph=graph, x=features)\n",
    "\n",
    "    preds = logits.argmax(axis=1)\n",
    "    \n",
    "    train_accuracy = (preds[train_idx] == labels[train_idx]).float().mean().item()\n",
    "    val_accuracy = (preds[val_idx] == labels[val_idx]).float().mean().item()\n",
    "    test_accuracy = (preds[test_idx] == labels[test_idx]).float().mean().item()\n",
    "    \n",
    "    metrics = {\n",
    "        'train accuracy': train_accuracy,\n",
    "        'val accuracy': val_accuracy,\n",
    "        'test accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_experiment(graph, features, labels, train_idx, val_idx, test_idx, graph_conv_module, num_layers=2,\n",
    "                   hidden_dim=256, num_heads=4, dropout=0.2, lr=3e-5, num_steps=500, device='cuda:0', amp=False):\n",
    "    model = Model(graph_conv_module=graph_conv_module,\n",
    "                  num_layers=num_layers,\n",
    "                  input_dim=features.shape[1],\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  output_dim=len(labels.unique()),\n",
    "                  num_heads=num_heads,\n",
    "                  dropout=dropout)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler(enabled=amp)\n",
    "    \n",
    "    graph = graph.to(device)\n",
    "    features = features.to(device)\n",
    "    labels = labels.to(device)\n",
    "    train_idx = train_idx.to(device)\n",
    "    val_idx = val_idx.to(device)\n",
    "    test_idx = test_idx.to(device)\n",
    "    \n",
    "    best_val_metric = 0\n",
    "    corresponding_test_metric = 0\n",
    "    best_step = None\n",
    "    with tqdm(total=num_steps) as progress_bar:\n",
    "        for step in range(1, num_steps + 1):\n",
    "            train_step(model=model, optimizer=optimizer, scaler=scaler, amp=amp, graph=graph, features=features,\n",
    "                       labels=labels, train_idx=train_idx)\n",
    "            metrics = evaluate(model=model, amp=amp, graph=graph, features=features, labels=labels,\n",
    "                               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx)\n",
    "\n",
    "            progress_bar.update()\n",
    "            progress_bar.set_postfix({metric: f'{value:.2f}' for metric, value in metrics.items()})\n",
    "            \n",
    "            if metrics['val accuracy'] > best_val_metric:\n",
    "                best_val_metric = metrics['val accuracy']\n",
    "                corresponding_test_metric = metrics['test accuracy']\n",
    "                best_step = step\n",
    "    \n",
    "    print(f'Best val accuracy: {best_val_metric:.4f}')\n",
    "    print(f'Corresponding test accuracy: {corresponding_test_metric:.4f}')\n",
    "    print(f'(step {best_step})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add8f70",
   "metadata": {},
   "source": [
    "This should look quite similar to your standard training loop, but with one notable difference - there are no mini-batches, we are always training on the whole graph. Since the data samples (graph nodes) are not independent, we cannot trivially sample a mini-batch.\n",
    "\n",
    "Now, let's implement a model. Don't forget about skip connections and layer normalization - they can signififcantly boost the performance of a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10515d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, dim, num_inputs, dropout):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_features=num_inputs * dim, out_features=dim)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        self.act = nn.GELU()\n",
    "        self.linear_2 = nn.Linear(in_features=dim, out_features=dim)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualModule(nn.Module):\n",
    "    def __init__(self, graph_conv_module, dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(normalized_shape=dim)\n",
    "        self.graph_conv = graph_conv_module(dim=dim, num_heads=num_heads)\n",
    "        self.feed_forward = FeedForwardModule(dim=dim, num_inputs=2, dropout=dropout)\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        x_res = self.normalization(x)\n",
    "        \n",
    "        x_aggregated = self.graph_conv(graph, x_res)\n",
    "        x_res = torch.cat([x_res, x_aggregated], axis=1)\n",
    "        \n",
    "        x_res = self.feed_forward(x_res)\n",
    "        \n",
    "        x = x + x_res\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, graph_conv_module, num_layers, input_dim, hidden_dim, output_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.input_linear = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.input_dropout = nn.Dropout(p=dropout)\n",
    "        self.input_act = nn.GELU()\n",
    "        \n",
    "        self.residual_modules = nn.ModuleList(\n",
    "            ResidualModule(graph_conv_module=graph_conv_module, dim=hidden_dim, num_heads=num_heads,\n",
    "                           dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        )\n",
    "        \n",
    "        self.output_normalization = nn.LayerNorm(hidden_dim)\n",
    "        self.output_linear = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        x = self.input_linear(x)\n",
    "        x = self.input_dropout(x)\n",
    "        x = self.input_act(x)\n",
    "        \n",
    "        for residual_module in self.residual_modules:\n",
    "            x = residual_module(graph, x)\n",
    "        \n",
    "        x = self.output_normalization(x)\n",
    "        logits = self.output_linear(x)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030a0f4",
   "metadata": {},
   "source": [
    "Now everything is ready - except for the graph convolution module. We will implement several variants of this module, which will constitute the only difference between our GNNs. But first - as a simple baseline - let's implement a graph convolution module that does nothing. It will allow us to see how a graph-agnostic model performs, so we can then compare our GNNs to this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2554025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        return torch.zeros_like(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = torch.empty(0)   # We don't care about graph representation for this experiment.\n",
    "\n",
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DummyGraphConv,\n",
    "               device=device, amp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b11368a",
   "metadata": {},
   "source": [
    "Now let's implement some real graph convolutions. Simple graph convolutions can be represented as operations with (sparse) matrices. Thus, they can be implemented in pure PyTorch. We will need the graph adjacency matrix $A$, the graph degree matrix $D$, and the matrix of node representations at layer $l$ $H^l$. Further, let $\\tilde{h_i}^{l}$ be the output of $\\mathrm{Aggregate}$ function at layer $l$ for node $v_i$ and let $\\widetilde{H}^l$ be the matrix of stacked vectors $\\tilde{h_i}^{l}$ for all nodes.\n",
    "\n",
    "For the next couple experiments, assume that the graph argument of the graph convolution forward method is a sparse adjacency matrix of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = torch.sparse_coo_tensor(indices=edges.T, values=torch.ones(len(edges)), size=(len(labels), len(labels)))\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c600c",
   "metadata": {},
   "source": [
    "Let's implement a graph convolution that simply takes the mean of neighbors' representations. We can write:\n",
    "\n",
    "$$ \\tilde{h}_i^{l+1} = \\frac{1}{|N(v_i)|} \\sum_{v_j \\in N(v_i)} h_j^l $$\n",
    "\n",
    "This operation can be written in matrix form:\n",
    "\n",
    "$$ \\widetilde{H}^{l+1} = D^{-1} A H^l $$\n",
    "\n",
    "Let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d71b93",
   "metadata": {},
   "source": [
    "(The computations can be sped up by precomputing $D^{-1} A$, but we won't do it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=MeanGraphConv,\n",
    "               device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefdb56c",
   "metadata": {},
   "source": [
    "As we can see, the accuracy is a lot better than in the previous experiment - our GNN works better than a graph-agnostoc model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698594f2",
   "metadata": {},
   "source": [
    "Now, let's try another simple GNN variant - this time we will implement a graph convolution proposed in [the GCN paper](https://arxiv.org/abs/1609.02907). The formula is:\n",
    "\n",
    "$$ \\tilde{h}_i^{l+1} = \\sum_{v_j \\in N(v_i)} \\frac{1}{\\sqrt{deg(v_i) deg(v_j)}} h_j^l $$\n",
    "\n",
    "It's very similar to the mean convolution, except we normalize each neighbor's representation not by the degree of the ego node, but by the geometric mean of the degree of the ego node and the neighbor. This operation can be written in matrix form:\n",
    "\n",
    "$$ \\widetilde{H}^{l+1} = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} H^l $$\n",
    "\n",
    "Let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01341bf6",
   "metadata": {},
   "source": [
    "(The computations can be sped up by precomputing $D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$, but we won't do it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40723c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=GCNGraphConv,\n",
    "               device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24658a84",
   "metadata": {},
   "source": [
    "The results are similar to those in the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65be773",
   "metadata": {},
   "source": [
    "Simple graph convolutions can be expressed as matrix operations, and thus, can be implemented in pure PyTorch. However, efficient implementation of more complex graph convolutions requires using specialized libraries. There are two most popular GNN libraries for PyTorch - [PyTorch Geometric (PyG)](https://github.com/pyg-team/pytorch_geometric) and [Deep Graph Library (DGL)](https://www.dgl.ai/). In this seminar, we will be using DGL, because ~it is objectively better~ the instructor likes it more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f583f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd4783",
   "metadata": {},
   "source": [
    "There are many features for deep learning on graphs in DGL, but we will only be using two of them - the Graph class, which is obviously used for representing a graph, and the [ops module](https://docs.dgl.ai/api/python/dgl.ops.html), which contains operators for message passing on graphs.\n",
    "\n",
    "First, let's create a graph representation which we will be using in the next few experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.graph((edges[:, 0], edges[:, 1]), num_nodes=len(labels))\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7a06e",
   "metadata": {},
   "source": [
    "Now let's reimplement the mean graph convolution, this time using DGL. For this we will need a certain operation from the ops module - can you guess which one by their names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50885ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGLMeanGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ad64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DGLMeanGraphConv,\n",
    "               device=device, amp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06023062",
   "metadata": {},
   "source": [
    "The results are roughly the same as for the pure PyTorch implementation, but the training is faster (graph message passing operations with DGL a generally faster than PyTorch sparse matrix multiplications, and, further, DGL supports using AMP with most of its operations, while PyTorch does not (yet) allow using AMP with sparse matrix operations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578c692",
   "metadata": {},
   "source": [
    "By simply swapping the ops.copy_u_mean function for the ops.copy_u_max function, we can get another graph convolution that computes the elementwise maximum of neighbors' representations. This one cannot be efficiently implemented in pure PyTorch. Let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4af522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGLMaxGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8135b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DGLMaxGraphConv,\n",
    "               device=device)   # This one currently does not work with AMP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a66478",
   "metadata": {},
   "source": [
    "Now, let's reimplement the GCN graph convolution using DGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGLGCNGraphConv(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0e78c",
   "metadata": {},
   "source": [
    "(The computations can be sped up by precomputing weights, but we won't do it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DGLGCNGraphConv,\n",
    "               device=device, amp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c623c2c",
   "metadata": {},
   "source": [
    "Now let's implement something more complex - the graph convolution proposed in [the GAT paper](https://arxiv.org/abs/1710.10903). This one uses attention (although a very simple version of it). The formulas are:\n",
    "\n",
    "$$ a_{ij} = \\mathrm{LeakyReLU} \\Big( w_1^T h_i^l + w_2^T h_j^l + b \\Big) $$\n",
    "\n",
    "$$ \\alpha_{ij} = \\mathrm{softmax}_{ij}(a_{i,1}, \\; a_{i,2}, \\; ... \\; a_{i, deg(v_i)}) = \\frac{\\mathrm{exp}(a_{ij})}{\\sum_{v_k \\in N(v_i)} \\mathrm{exp}(a_{ik})} $$\n",
    "\n",
    "$$ \\tilde{h}_i^{l+1} = \\sum_{v_j \\in N(v_i)} \\alpha_{ij} h_j^l $$\n",
    "\n",
    "where $\\mathrm{softmax}_{ij}(a_{i,1}, \\; a_{i,2}, \\; ... \\; a_{i, deg(v_i)})$ is the $j$-th output of softmax of the values $a_{ik}$ corresponding to the neighbors of the node $v_i$, i.e., softmax is taken only over the ego node's neighborhood. This function is available in DGL.\n",
    "\n",
    "Note that additionally the attention mechanism is multi-headed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36560d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.functional import edge_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9bf61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGLGATGraphConv(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, **kwargs):\n",
    "        super().__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################\n",
    "    \n",
    "    def forward(self, graph, x):\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################\n",
    "        \n",
    "        return x_aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a402962",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(graph=graph, features=features, labels=labels,\n",
    "               train_idx=train_idx, val_idx=val_idx, test_idx=test_idx,\n",
    "               graph_conv_module=DGLGATGraphConv,\n",
    "               device=device, amp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e036ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
